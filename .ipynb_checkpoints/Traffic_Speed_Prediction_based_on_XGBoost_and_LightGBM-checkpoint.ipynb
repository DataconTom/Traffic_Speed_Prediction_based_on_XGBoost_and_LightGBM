{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The text.latex.unicode rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The savefig.frameon rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The pgf.debug rcparam was deprecated in Matplotlib 3.0 and will be removed in 3.2.\n",
      "In C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.level rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n",
      "In C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test.mplstyle: \n",
      "The verbose.fileo rcparam was deprecated in Matplotlib 3.1 and will be removed in 3.3.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as skl\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, Imputer\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime, date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate Training Set and Test Set: Basic Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set time stamp as the index and sort it by the order \n",
    "\n",
    "#1. training set\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df.isnull().any()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(by = 'date')\n",
    "df = df.set_index('date')\n",
    "\n",
    "#2. test set\n",
    "ts = pd.read_csv(\"test.csv\")\n",
    "ts.isnull().any()\n",
    "ts['date'] = pd.to_datetime(ts['date'])\n",
    "ts = ts.sort_values(by = 'date')\n",
    "ts = ts.set_index('date')\n",
    "ts_temp = ts\n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hong Kong holiday information\n",
    "holiday_2017 = [(1,2),(1,28),(1,30),(1,31),(4,4),(4,14),(4,15),(4,17),(5,1),(5,3),(5,30),(7,1),(10,2),(10,5),(10,28),(12,25),(12,26)]\n",
    "holiday_2018 = [(1,1),(2,16),(2,17),(2,19),(3,30),(3,31),(4,2),(4,5),(5,1),(5,22),(6,18),(7,2),(9,25),(10,1),(10,17),(12,25),(12,26)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# holiday variable: judge whether this day is holiday or not by checking the holiday array \n",
    "def f_holiday(x):\n",
    "    if int(x.split(\"-\")[0]) == 2017:\n",
    "        temp = (int(x.split(\"-\")[1]),int(x.split(\"-\")[2]))\n",
    "        return 1 if temp in holiday_2017 else 0\n",
    "    if int(x.split(\"-\")[0]) == 2018:\n",
    "        temp = (int(x.split(\"-\")[1]),int(x.split(\"-\")[2]))\n",
    "        return 1 if temp in holiday_2018 else 0\n",
    "    \n",
    "\n",
    "# weekday variable: check whether this day is sunday or not\n",
    "def f_weekday(x):\n",
    "    return datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").weekday()\n",
    "\n",
    "\n",
    "# workday variable: form the dummy variable for working days and sunday\n",
    "def f_workday(x):\n",
    "    return 1 if x < 6 else 0\n",
    "def f_sunday(x):\n",
    "    return 1 if x >= 6 else 0\n",
    "\n",
    "\n",
    "# work-night hour variable\n",
    "def f_work_hour(x):\n",
    "    return 1 if x>=7 and x<=19 else 0\n",
    "def f_night_hour(x):\n",
    "    return 1 if x<7 or x>19 else 0\n",
    "\n",
    "\n",
    "# month period variable\n",
    "def f_month_period0_divide(x):\n",
    "    return 1 if x<=10 else 0\n",
    "def f_month_period1_divide(x):\n",
    "    return 1 if x>10 and x<=20 else 0\n",
    "def f_month_period2_divide(x):\n",
    "    return 1 if x>20 else 0\n",
    "\n",
    "\n",
    "# season divide variable\n",
    "def f_season0_divide(x):\n",
    "    return 1 if x>=1 and x<=3 else 0\n",
    "def f_season1_divide(x):\n",
    "    return 1 if x>=4 and x<=6 else 0\n",
    "def f_season2_divide(x):\n",
    "    return 1 if x>=7 and x<=9 else 0\n",
    "def f_season3_divide(x):\n",
    "    return 1 if x>=10 and x<=12 else 0\n",
    "\n",
    "\n",
    "# ith week variable in its month: \n",
    "# find the day correponding to which week (Ath week) in this year \n",
    "# then find the 1th day of its month correponding to which week (Bth week) in this year \n",
    "# then the A-B+1 th week is the answer \n",
    "def f_week_of_month(year, month, day):\n",
    "    begin = int(date(int(year), int(month), 1).strftime(\"%W\"))\n",
    "    end = int(date(int(year), int(month), int(day)).strftime(\"%W\"))\n",
    "    return end - begin + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weekday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>workday</th>\n",
       "      <th>sunday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>work_hour</th>\n",
       "      <th>night_hour</th>\n",
       "      <th>month_period0_divide</th>\n",
       "      <th>month_period1_divide</th>\n",
       "      <th>month_period2_divide</th>\n",
       "      <th>season0_divide</th>\n",
       "      <th>season1_divide</th>\n",
       "      <th>season2_divide</th>\n",
       "      <th>season3_divide</th>\n",
       "      <th>week_of_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-01 02:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 05:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 07:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 08:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-01 10:00:00</th>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     weekday  year  month  day  hour  workday  sunday  \\\n",
       "date                                                                    \n",
       "2018-01-01 02:00:00        0  2018      1    1     2        1       0   \n",
       "2018-01-01 05:00:00        0  2018      1    1     5        1       0   \n",
       "2018-01-01 07:00:00        0  2018      1    1     7        1       0   \n",
       "2018-01-01 08:00:00        0  2018      1    1     8        1       0   \n",
       "2018-01-01 10:00:00        0  2018      1    1    10        1       0   \n",
       "\n",
       "                     holiday  work_hour  night_hour  month_period0_divide  \\\n",
       "date                                                                        \n",
       "2018-01-01 02:00:00        1          0           1                     1   \n",
       "2018-01-01 05:00:00        1          0           1                     1   \n",
       "2018-01-01 07:00:00        1          1           0                     1   \n",
       "2018-01-01 08:00:00        1          1           0                     1   \n",
       "2018-01-01 10:00:00        1          1           0                     1   \n",
       "\n",
       "                     month_period1_divide  month_period2_divide  \\\n",
       "date                                                              \n",
       "2018-01-01 02:00:00                     0                     0   \n",
       "2018-01-01 05:00:00                     0                     0   \n",
       "2018-01-01 07:00:00                     0                     0   \n",
       "2018-01-01 08:00:00                     0                     0   \n",
       "2018-01-01 10:00:00                     0                     0   \n",
       "\n",
       "                     season0_divide  season1_divide  season2_divide  \\\n",
       "date                                                                  \n",
       "2018-01-01 02:00:00               1               0               0   \n",
       "2018-01-01 05:00:00               1               0               0   \n",
       "2018-01-01 07:00:00               1               0               0   \n",
       "2018-01-01 08:00:00               1               0               0   \n",
       "2018-01-01 10:00:00               1               0               0   \n",
       "\n",
       "                     season3_divide  week_of_month  \n",
       "date                                                \n",
       "2018-01-01 02:00:00               0              1  \n",
       "2018-01-01 05:00:00               0              1  \n",
       "2018-01-01 07:00:00               0              1  \n",
       "2018-01-01 08:00:00               0              1  \n",
       "2018-01-01 10:00:00               0              1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing the time stamp variables into corresponding variables \n",
    "ts.index = ts.index.map(str)\n",
    "ts['weekday'] = ts.index.map(lambda x: f_weekday(str(x)))\n",
    "ts_workday_data = ts\n",
    "ts_workday_data.index = ts_workday_data.index.map(str)\n",
    "ts_workday_data['days'] = ts_workday_data.index.map(lambda x:x.split(\" \")[0])\n",
    "ts_workday_data['hours'] = ts_workday_data.index.map(lambda x:x.split(\" \")[1])\n",
    "ts_workday_data['year'] = ts_workday_data['days'].map(lambda x:x.split(\"-\")[0])\n",
    "ts_workday_data['month'] = ts_workday_data['days'].map(lambda x:x.split(\"-\")[1])\n",
    "ts_workday_data['day'] = ts_workday_data['days'].map(lambda x:x.split(\"-\")[2])\n",
    "ts_workday_data['hour'] = ts_workday_data['hours'].map(lambda x:x.split(\":\")[0])\n",
    "ts_workday_data = ts_workday_data.astype({\"hour\": int, \"day\": int, \"month\":int, \"year\":int})\n",
    "\n",
    "\n",
    "# workday variable\n",
    "ts_workday_data['workday'] = ts_workday_data['weekday'].map(lambda x: f_workday(x))\n",
    "ts_workday_data['sunday'] = ts_workday_data['weekday'].map(lambda x: f_sunday(x))\n",
    "ts_workday_data['holiday'] = ts_workday_data['days'].map(lambda x: f_holiday(x))\n",
    "    \n",
    "\n",
    "# work-night hour dummy variable    \n",
    "ts_workday_data['work_hour'] = ts_workday_data['hour'].map(lambda x: f_work_hour(x))\n",
    "ts_workday_data['night_hour'] = ts_workday_data['hour'].map(lambda x: f_night_hour(x))\n",
    "\n",
    "\n",
    "# month period dummy variable: \n",
    "ts_workday_data['month_period0_divide'] = ts_workday_data['day'].map(lambda x: f_month_period0_divide(x))\n",
    "ts_workday_data['month_period1_divide'] = ts_workday_data['day'].map(lambda x: f_month_period1_divide(x))\n",
    "ts_workday_data['month_period2_divide'] = ts_workday_data['day'].map(lambda x: f_month_period2_divide(x))\n",
    "\n",
    "\n",
    "# season dummy variable\n",
    "ts_workday_data['season0_divide'] = ts_workday_data['month'].map(lambda x: f_season0_divide(x))\n",
    "ts_workday_data['season1_divide'] = ts_workday_data['month'].map(lambda x: f_season1_divide(x))\n",
    "ts_workday_data['season2_divide'] = ts_workday_data['month'].map(lambda x: f_season2_divide(x))\n",
    "ts_workday_data['season3_divide'] = ts_workday_data['month'].map(lambda x: f_season3_divide(x))\n",
    "\n",
    "\n",
    "# ith week variable in its month \n",
    "ts_workday_data['week_of_month'] = ts_workday_data['days'].map(lambda x: f_week_of_month(x.split(\"-\")[0], x.split(\"-\")[1], x.split(\"-\")[2]))\n",
    "\n",
    "\n",
    "# dropping the some un-related variables\n",
    "ts_workday_data = ts_workday_data.drop('hours', 1)\n",
    "ts_workday_data = ts_workday_data.drop('days', 1)\n",
    "ts_workday_data = ts_workday_data.drop('id',1)\n",
    "test_workday_temp = ts_workday_data\n",
    "test_workday_temp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speed</th>\n",
       "      <th>weekday</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>workday</th>\n",
       "      <th>sunday</th>\n",
       "      <th>holiday</th>\n",
       "      <th>work_hour</th>\n",
       "      <th>night_hour</th>\n",
       "      <th>month_period0_divide</th>\n",
       "      <th>month_period1_divide</th>\n",
       "      <th>month_period2_divide</th>\n",
       "      <th>season0_divide</th>\n",
       "      <th>season1_divide</th>\n",
       "      <th>season2_divide</th>\n",
       "      <th>season3_divide</th>\n",
       "      <th>week_of_month</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <td>43.002930</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 01:00:00</th>\n",
       "      <td>46.118696</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 02:00:00</th>\n",
       "      <td>44.294158</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 03:00:00</th>\n",
       "      <td>41.067468</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 04:00:00</th>\n",
       "      <td>46.448653</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         speed  weekday  year  month  day  hour  workday  \\\n",
       "date                                                                       \n",
       "2017-01-01 00:00:00  43.002930        6  2017      1    1     0        0   \n",
       "2017-01-01 01:00:00  46.118696        6  2017      1    1     1        0   \n",
       "2017-01-01 02:00:00  44.294158        6  2017      1    1     2        0   \n",
       "2017-01-01 03:00:00  41.067468        6  2017      1    1     3        0   \n",
       "2017-01-01 04:00:00  46.448653        6  2017      1    1     4        0   \n",
       "\n",
       "                     sunday  holiday  work_hour  night_hour  \\\n",
       "date                                                          \n",
       "2017-01-01 00:00:00       1        0          0           1   \n",
       "2017-01-01 01:00:00       1        0          0           1   \n",
       "2017-01-01 02:00:00       1        0          0           1   \n",
       "2017-01-01 03:00:00       1        0          0           1   \n",
       "2017-01-01 04:00:00       1        0          0           1   \n",
       "\n",
       "                     month_period0_divide  month_period1_divide  \\\n",
       "date                                                              \n",
       "2017-01-01 00:00:00                     1                     0   \n",
       "2017-01-01 01:00:00                     1                     0   \n",
       "2017-01-01 02:00:00                     1                     0   \n",
       "2017-01-01 03:00:00                     1                     0   \n",
       "2017-01-01 04:00:00                     1                     0   \n",
       "\n",
       "                     month_period2_divide  season0_divide  season1_divide  \\\n",
       "date                                                                        \n",
       "2017-01-01 00:00:00                     0               1               0   \n",
       "2017-01-01 01:00:00                     0               1               0   \n",
       "2017-01-01 02:00:00                     0               1               0   \n",
       "2017-01-01 03:00:00                     0               1               0   \n",
       "2017-01-01 04:00:00                     0               1               0   \n",
       "\n",
       "                     season2_divide  season3_divide  week_of_month  \n",
       "date                                                                \n",
       "2017-01-01 00:00:00               0               0              1  \n",
       "2017-01-01 01:00:00               0               0              1  \n",
       "2017-01-01 02:00:00               0               0              1  \n",
       "2017-01-01 03:00:00               0               0              1  \n",
       "2017-01-01 04:00:00               0               0              1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing the time stamp variables into corresponding variables \n",
    "df.index = df.index.map(str)\n",
    "df['weekday'] = df.index.map(lambda x: f_weekday(str(x)))\n",
    "train_sunday_data = df\n",
    "train_sunday_data.index = train_sunday_data.index.map(str)\n",
    "train_sunday_data['days'] = train_sunday_data.index.map(lambda x:x.split(\" \")[0])\n",
    "train_sunday_data['hours'] = train_sunday_data.index.map(lambda x:x.split(\" \")[1])\n",
    "train_sunday_data['year'] = train_sunday_data['days'].map(lambda x:x.split(\"-\")[0])\n",
    "train_sunday_data['month'] = train_sunday_data['days'].map(lambda x:x.split(\"-\")[1])\n",
    "train_sunday_data['day'] = train_sunday_data['days'].map(lambda x:x.split(\"-\")[2])\n",
    "train_sunday_data['hour'] = train_sunday_data['hours'].map(lambda x:x.split(\":\")[0])\n",
    "train_sunday_data = train_sunday_data.astype({\"hour\": int, \"day\": int, \"month\":int, \"year\":int})\n",
    "\n",
    "\n",
    "# workday variable\n",
    "train_sunday_data['workday'] = train_sunday_data['weekday'].map(lambda x: f_workday(x))\n",
    "train_sunday_data['sunday'] = train_sunday_data['weekday'].map(lambda x: f_sunday(x))\n",
    "train_sunday_data['holiday'] = train_sunday_data['days'].map(lambda x: f_holiday(x))\n",
    "\n",
    "# work-night hour dummy variable\n",
    "train_sunday_data['work_hour'] = train_sunday_data['hour'].map(lambda x: f_work_hour(x))\n",
    "train_sunday_data['night_hour'] = train_sunday_data['hour'].map(lambda x: f_night_hour(x))\n",
    "\n",
    "\n",
    "# month period dummy variable: \n",
    "train_sunday_data['month_period0_divide'] = train_sunday_data['day'].map(lambda x: f_month_period0_divide(x))\n",
    "train_sunday_data['month_period1_divide'] = train_sunday_data['day'].map(lambda x: f_month_period1_divide(x))\n",
    "train_sunday_data['month_period2_divide'] = train_sunday_data['day'].map(lambda x: f_month_period2_divide(x))\n",
    "\n",
    "\n",
    "# season dummy variable\n",
    "train_sunday_data['season0_divide'] = train_sunday_data['month'].map(lambda x: f_season0_divide(x))\n",
    "train_sunday_data['season1_divide'] = train_sunday_data['month'].map(lambda x: f_season1_divide(x))\n",
    "train_sunday_data['season2_divide'] = train_sunday_data['month'].map(lambda x: f_season2_divide(x))\n",
    "train_sunday_data['season3_divide'] = train_sunday_data['month'].map(lambda x: f_season3_divide(x))\n",
    "\n",
    "# ith week variable in its month \n",
    "train_sunday_data['week_of_month'] = train_sunday_data['days'].map(lambda x: f_week_of_month(x.split(\"-\")[0], x.split(\"-\")[1], x.split(\"-\")[2]))\n",
    "\n",
    "\n",
    "# dropping the some un-related variables \n",
    "train_sunday_data = train_sunday_data.drop('hours', 1)\n",
    "train_sunday_data = train_sunday_data.drop('days', 1)\n",
    "train_sunday_data = train_sunday_data.drop('id',1)\n",
    "\n",
    "\n",
    "train_sunday_temp = train_sunday_data\n",
    "train_sunday_temp.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing Training Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_sunday_temp[\"speed\"]\n",
    "train_sunday_temp.drop([\"speed\"],axis=1,inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_sunday_temp, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost--Directly Use RandomizedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 14.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bynode=1, colsample_bytree=0.9, gamma=0.6, gpu_id=-1,\n",
      "       importance_type='gain', interaction_constraints='',\n",
      "       learning_rate=0.05, max_delta_step=0, max_depth=100,\n",
      "       min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "       n_estimators=600, n_jobs=4, nthread=4, num_parallel_tree=1,\n",
      "       objective='reg:squarederror', random_state=0, reg_alpha=1,\n",
      "       reg_lambda=2, scale_pos_weight=1, subsample=0.6,\n",
      "       tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "-14.350788604335628\n"
     ]
    }
   ],
   "source": [
    "# import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "xgb1 = XGBRegressor()\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              #'objective':['reg:linear'],\n",
    "              'learning_rate': [0.001,.03, 0.05,.01,.1], #so called `eta` value\n",
    "              'max_depth': [5,10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    "              'min_child_weight': [1,2,3,4,5,6,7,8,9,10],\n",
    "              #'silent': [1],\n",
    "              'subsample': [0.6, 0.7, 0.8, 0.9],\n",
    "              'colsample_bytree': [0.6, 0.7, 0.8, 0.9],\n",
    "              'n_estimators': [400,600,800,1000,2000,3000],\n",
    "              'gamma':[0.1,0.3,0.5,0.6],\n",
    "              'reg_alpha': [0.05, 0.1, 1, 2], \n",
    "              'reg_lambda': [0.05, 0.1, 1, 2]}\n",
    "\n",
    "clf = RandomizedSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 10,\n",
    "                        n_jobs = -1,\n",
    "                        verbose=True,scoring = \"neg_mean_squared_error\")\n",
    "clf.fit(X_train,y_train)\n",
    "print(clf.best_estimator_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost--Directly Use RandomizedSearch--Prediction and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using best parameters got by the above RandomizedSearch to fit the model \n",
    "model = xgb.XGBRegressor(learning_rate=0.03, n_estimators=300, max_depth=80, min_child_weight=1, seed=0,\n",
    "                             subsample=0.7, colsample_bytree=0.8, gamma=0.1, reg_alpha=0.05, reg_lambda=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_workday_temp)\n",
    "\n",
    "\n",
    "# (id, speed prediction) is the formate we need to save into result.csv file\n",
    "test_workday_temp['speed'] = y_pred\n",
    "day_pred = test_workday_temp['speed']\n",
    "df_test = ts_temp.join(day_pred)\n",
    "df_test = df_test.set_index('id')\n",
    "\n",
    "\n",
    "df_test.to_csv(\"test.csv\", columns=[\"speed\"], index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost--Step by Step to Use GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  6.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[mean: -16.22691, std: 1.24356, params: {'n_estimators': 2200}, mean: -16.23628, std: 1.23540, params: {'n_estimators': 2250}, mean: -16.21842, std: 1.21813, params: {'n_estimators': 2300}, mean: -16.22455, std: 1.22429, params: {'n_estimators': 2350}, mean: -16.22764, std: 1.23199, params: {'n_estimators': 2400}, mean: -16.22455, std: 1.23290, params: {'n_estimators': 2450}]\n",
      "参数的最佳取值：{'n_estimators': 2300}\n",
      "最佳模型得分:-16.21842076036648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 1. find the best value：n_estimators\n",
    "#  cv_params = {'n_estimators': [550, 575, 600, 650, 675]}\n",
    "#     other_params = {'learning_rate': 0.1, 'n_estimators': 600, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,\n",
    "#                     'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "\n",
    "# initiate parameters \n",
    "cv_params = {'n_estimators': [2200,2250,2300,2350,2400,2450]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 2300, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,\n",
    "               'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "\n",
    "# use the GridSearch find the best value\n",
    "model = xgb.XGBRegressor(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=-1)\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "evalute_result = optimized_GBM.grid_scores_\n",
    "\n",
    "# print the result \n",
    "print('final running result:{0}'.format(evalute_result))\n",
    "print('best value：{0}'.format(optimized_GBM.best_params_))\n",
    "print('best model score:{0}'.format(optimized_GBM.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 15 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=4)]: Done 150 out of 150 | elapsed: 30.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[mean: -15.27619, std: 1.37786, params: {'max_depth': 15, 'min_child_weight': 1}, mean: -15.48134, std: 1.49105, params: {'max_depth': 15, 'min_child_weight': 2}, mean: -15.75628, std: 1.41771, params: {'max_depth': 15, 'min_child_weight': 3}, mean: -15.30299, std: 1.42983, params: {'max_depth': 16, 'min_child_weight': 1}, mean: -15.50522, std: 1.52878, params: {'max_depth': 16, 'min_child_weight': 2}, mean: -15.63908, std: 1.38597, params: {'max_depth': 16, 'min_child_weight': 3}, mean: -15.17177, std: 1.35004, params: {'max_depth': 17, 'min_child_weight': 1}, mean: -15.53803, std: 1.49080, params: {'max_depth': 17, 'min_child_weight': 2}, mean: -15.68627, std: 1.44307, params: {'max_depth': 17, 'min_child_weight': 3}, mean: -15.23814, std: 1.36516, params: {'max_depth': 18, 'min_child_weight': 1}, mean: -15.52335, std: 1.35798, params: {'max_depth': 18, 'min_child_weight': 2}, mean: -15.70196, std: 1.36968, params: {'max_depth': 18, 'min_child_weight': 3}, mean: -15.30348, std: 1.35049, params: {'max_depth': 19, 'min_child_weight': 1}, mean: -15.52429, std: 1.55886, params: {'max_depth': 19, 'min_child_weight': 2}, mean: -15.74028, std: 1.34005, params: {'max_depth': 19, 'min_child_weight': 3}]\n",
      "参数的最佳取值：{'max_depth': 17, 'min_child_weight': 1}\n",
      "最佳模型得分:-15.171766336958664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 2.find the best values: min_child_weight and max_depth\n",
    "# initiate parameters\n",
    "cv_params = {'max_depth': [15,16,17,18,19], 'min_child_weight': [1, 2, 3]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 2300, 'max_depth': 17, 'min_child_weight': 1, 'seed': 0,\n",
    "                'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "\n",
    "# use the GridSearch find the best value\n",
    "model = xgb.XGBRegressor(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "evalute_result = optimized_GBM.grid_scores_\n",
    "\n",
    "# print the result \n",
    "print('final running result:{0}'.format(evalute_result))\n",
    "print('best value：{0}'.format(optimized_GBM.best_params_))\n",
    "print('best model score:{0}'.format(optimized_GBM.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 22.5min\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 30.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[mean: -15.18117, std: 1.39399, params: {'gamma': 0.1}, mean: -15.16770, std: 1.31661, params: {'gamma': 0.2}, mean: -15.20673, std: 1.41309, params: {'gamma': 0.3}, mean: -15.22961, std: 1.36560, params: {'gamma': 0.4}, mean: -15.25166, std: 1.40444, params: {'gamma': 0.5}, mean: -15.28326, std: 1.37211, params: {'gamma': 0.6}]\n",
      "参数的最佳取值：{'gamma': 0.2}\n",
      "最佳模型得分:-15.167704314669633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 3. finding the best value：gamma\n",
    "cv_params = {'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 2300, 'max_depth': 17, 'min_child_weight': 1, 'seed': 0,\n",
    "                'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "\n",
    "# use the GridSearch find the best value\n",
    "model = xgb.XGBRegressor(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "evalute_result = optimized_GBM.grid_scores_\n",
    "\n",
    "# print the result \n",
    "print('final running result:{0}'.format(evalute_result))\n",
    "print('best value：{0}'.format(optimized_GBM.best_params_))\n",
    "print('best model score:{0}'.format(optimized_GBM.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 16 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 13.5min\n",
      "[Parallel(n_jobs=4)]: Done 160 out of 160 | elapsed: 66.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[mean: -16.11430, std: 1.38107, params: {'colsample_bytree': 0.6, 'subsample': 0.6}, mean: -15.98434, std: 1.44508, params: {'colsample_bytree': 0.6, 'subsample': 0.7}, mean: -16.10569, std: 1.37369, params: {'colsample_bytree': 0.6, 'subsample': 0.8}, mean: -16.11512, std: 1.28199, params: {'colsample_bytree': 0.6, 'subsample': 0.9}, mean: -15.54815, std: 1.48717, params: {'colsample_bytree': 0.7, 'subsample': 0.6}, mean: -15.63538, std: 1.51108, params: {'colsample_bytree': 0.7, 'subsample': 0.7}, mean: -15.40615, std: 1.26059, params: {'colsample_bytree': 0.7, 'subsample': 0.8}, mean: -15.71203, std: 1.41871, params: {'colsample_bytree': 0.7, 'subsample': 0.9}, mean: -15.34967, std: 1.47089, params: {'colsample_bytree': 0.8, 'subsample': 0.6}, mean: -15.39851, std: 1.56859, params: {'colsample_bytree': 0.8, 'subsample': 0.7}, mean: -15.16770, std: 1.31661, params: {'colsample_bytree': 0.8, 'subsample': 0.8}, mean: -15.50576, std: 1.58135, params: {'colsample_bytree': 0.8, 'subsample': 0.9}, mean: -15.34323, std: 1.43001, params: {'colsample_bytree': 0.9, 'subsample': 0.6}, mean: -15.46413, std: 1.45255, params: {'colsample_bytree': 0.9, 'subsample': 0.7}, mean: -15.56922, std: 1.39307, params: {'colsample_bytree': 0.9, 'subsample': 0.8}, mean: -15.83896, std: 1.51641, params: {'colsample_bytree': 0.9, 'subsample': 0.9}]\n",
      "参数的最佳取值：{'colsample_bytree': 0.8, 'subsample': 0.8}\n",
      "最佳模型得分:-15.167704314669633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 4、finding the best values: subsample and colsample_bytree\n",
    "cv_params = {'subsample': [0.6, 0.7, 0.8, 0.9], 'colsample_bytree': [0.6, 0.7, 0.8, 0.9]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 2300, 'max_depth': 17, 'min_child_weight': 1, 'seed': 0,\n",
    "                'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0, 'reg_lambda': 1}\n",
    "\n",
    "# use the GridSearch find the best value\n",
    "model = xgb.XGBRegressor(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "evalute_result = optimized_GBM.grid_scores_\n",
    "\n",
    "# print the result \n",
    "print('final running result:{0}'.format(evalute_result))\n",
    "print('best value：{0}'.format(optimized_GBM.best_params_))\n",
    "print('best model score:{0}'.format(optimized_GBM.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 25 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 79.9min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed: 103.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[mean: -15.29405, std: 1.33961, params: {'reg_alpha': 0.03, 'reg_lambda': 1}, mean: -14.57189, std: 1.50178, params: {'reg_alpha': 0.03, 'reg_lambda': 2}, mean: -14.37725, std: 1.39864, params: {'reg_alpha': 0.03, 'reg_lambda': 3}, mean: -14.45982, std: 1.28773, params: {'reg_alpha': 0.03, 'reg_lambda': 4}, mean: -14.44591, std: 1.38203, params: {'reg_alpha': 0.03, 'reg_lambda': 5}, mean: -15.28520, std: 1.39825, params: {'reg_alpha': 0.04, 'reg_lambda': 1}, mean: -14.63542, std: 1.45204, params: {'reg_alpha': 0.04, 'reg_lambda': 2}, mean: -14.40583, std: 1.42825, params: {'reg_alpha': 0.04, 'reg_lambda': 3}, mean: -14.52798, std: 1.26264, params: {'reg_alpha': 0.04, 'reg_lambda': 4}, mean: -14.43247, std: 1.37781, params: {'reg_alpha': 0.04, 'reg_lambda': 5}, mean: -15.24558, std: 1.33713, params: {'reg_alpha': 0.05, 'reg_lambda': 1}, mean: -14.64144, std: 1.40620, params: {'reg_alpha': 0.05, 'reg_lambda': 2}, mean: -14.29069, std: 1.34953, params: {'reg_alpha': 0.05, 'reg_lambda': 3}, mean: -14.44453, std: 1.33706, params: {'reg_alpha': 0.05, 'reg_lambda': 4}, mean: -14.37894, std: 1.35121, params: {'reg_alpha': 0.05, 'reg_lambda': 5}, mean: -15.27170, std: 1.37696, params: {'reg_alpha': 0.06, 'reg_lambda': 1}, mean: -14.68921, std: 1.42962, params: {'reg_alpha': 0.06, 'reg_lambda': 2}, mean: -14.41199, std: 1.36332, params: {'reg_alpha': 0.06, 'reg_lambda': 3}, mean: -14.48671, std: 1.40356, params: {'reg_alpha': 0.06, 'reg_lambda': 4}, mean: -14.38897, std: 1.39155, params: {'reg_alpha': 0.06, 'reg_lambda': 5}, mean: -15.26359, std: 1.42743, params: {'reg_alpha': 0.07, 'reg_lambda': 1}, mean: -14.56937, std: 1.45026, params: {'reg_alpha': 0.07, 'reg_lambda': 2}, mean: -14.35481, std: 1.31924, params: {'reg_alpha': 0.07, 'reg_lambda': 3}, mean: -14.53175, std: 1.35602, params: {'reg_alpha': 0.07, 'reg_lambda': 4}, mean: -14.41435, std: 1.29612, params: {'reg_alpha': 0.07, 'reg_lambda': 5}]\n",
      "参数的最佳取值：{'reg_alpha': 0.05, 'reg_lambda': 3}\n",
      "最佳模型得分:-14.290686825519304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 5、finding the best value：reg_alpha and reg_lambda：\n",
    "cv_params = {'reg_alpha': [0.03, 0.04, 0.05, 0.06, 0.07], 'reg_lambda': [ 1, 2, 3, 4, 5]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 2300, 'max_depth': 17, 'min_child_weight': 1, 'seed': 0,\n",
    "                'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.05, 'reg_lambda': 3}\n",
    "\n",
    "# use the GridSearch find the best value\n",
    "model = xgb.XGBRegressor(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "evalute_result = optimized_GBM.grid_scores_\n",
    "\n",
    "# print the result \n",
    "print('final running result:{0}'.format(evalute_result))\n",
    "print('best value：{0}'.format(optimized_GBM.best_params_))\n",
    "print('best model score:{0}'.format(optimized_GBM.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=4)]: Done  60 out of  60 | elapsed: 28.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每轮迭代运行结果:[mean: -14.11122, std: 1.32832, params: {'learning_rate': 0.01}, mean: -14.31344, std: 1.37052, params: {'learning_rate': 0.03}, mean: -14.23997, std: 1.29115, params: {'learning_rate': 0.05}, mean: -14.34551, std: 1.35486, params: {'learning_rate': 0.07}, mean: -14.29069, std: 1.34953, params: {'learning_rate': 0.1}, mean: -14.77803, std: 1.42986, params: {'learning_rate': 0.2}]\n",
      "参数的最佳取值：{'learning_rate': 0.01}\n",
      "最佳模型得分:-14.111222797089477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "## 6、finding the best value: learning_rate (choose some small values)\n",
    "cv_params = {'learning_rate': [0.01,0.02, 0.03,0.04]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 2300, 'max_depth': 17, 'min_child_weight': 1, 'seed': 0,\n",
    "                'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.2, 'reg_alpha': 0.05, 'reg_lambda':3}\n",
    "\n",
    "# use the GridSearch find the best value\n",
    "model = xgb.XGBRegressor(**other_params)\n",
    "optimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "optimized_GBM.fit(X_train, y_train)\n",
    "evalute_result = optimized_GBM.grid_scores_\n",
    "\n",
    "# print the result \n",
    "print('final running result:{0}'.format(evalute_result))\n",
    "print('best value：{0}'.format(optimized_GBM.best_params_))\n",
    "print('best model score:{0}'.format(optimized_GBM.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost--Step by Step to Use GridSearch--Prediction and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using best parameters got by the above Step by Step GridSearch to fit the model \n",
    "model = xgb.XGBRegressor(learning_rate=0.06, n_estimators=280, max_depth=20, min_child_weight=1, seed=0,\n",
    "                             subsample=0.8, colsample_bytree=0.9, gamma=0.3, reg_alpha=5, reg_lambda=2)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(test_workday_temp)\n",
    "\n",
    "\n",
    "# (id, speed prediction) is the formate we need to save into result.csv file\n",
    "test_workday_temp['speed'] = y_pred\n",
    "day_pred = test_workday_temp['speed']\n",
    "df_test = ts_temp.join(day_pred)\n",
    "df_test = df_test.set_index('id')\n",
    "\n",
    "\n",
    "df_test.to_csv(\"test.csv\", columns=[\"speed\"], index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM--Step by Step to Use GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001220 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000716 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[50]\tcv_agg's rmse: 5.21437 + 0.120204\n",
      "[100]\tcv_agg's rmse: 4.86944 + 0.150736\n",
      "[150]\tcv_agg's rmse: 4.69055 + 0.13506\n",
      "[200]\tcv_agg's rmse: 4.55133 + 0.116869\n",
      "[250]\tcv_agg's rmse: 4.41847 + 0.121815\n",
      "[300]\tcv_agg's rmse: 4.34786 + 0.118774\n",
      "[350]\tcv_agg's rmse: 4.25695 + 0.103537\n",
      "[400]\tcv_agg's rmse: 4.21468 + 0.0973281\n",
      "[450]\tcv_agg's rmse: 4.19057 + 0.0939745\n",
      "[500]\tcv_agg's rmse: 4.16018 + 0.0950812\n",
      "[550]\tcv_agg's rmse: 4.14825 + 0.10158\n",
      "[600]\tcv_agg's rmse: 4.12917 + 0.101027\n",
      "[650]\tcv_agg's rmse: 4.1124 + 0.100383\n",
      "[700]\tcv_agg's rmse: 4.10205 + 0.0994271\n",
      "[750]\tcv_agg's rmse: 4.08437 + 0.102919\n",
      "[800]\tcv_agg's rmse: 4.07028 + 0.10277\n",
      "[850]\tcv_agg's rmse: 4.06657 + 0.101741\n",
      "[900]\tcv_agg's rmse: 4.06373 + 0.103247\n",
      "[950]\tcv_agg's rmse: 4.05679 + 0.10314\n",
      "[1000]\tcv_agg's rmse: 4.05514 + 0.102101\n",
      "best n_estimators: 991\n",
      "best cv score: 4.054697386909495\n"
     ]
    }
   ],
   "source": [
    "# 1. finding the best value: n_estimator \n",
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'regression', \n",
    "    'learning_rate': 0.1, \n",
    "    'num_leaves': 50, \n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.8, \n",
    "}\n",
    "\n",
    "data_train = lgb.Dataset(X_train, y_train, silent=True)\n",
    "cv_results = lgb.cv(\n",
    "    params, data_train, num_boost_round=1000, nfold=5, stratified=False, shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=50, verbose_eval=50, show_stdv=True, seed=0)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
    "print('best cv score:', cv_results['rmse-mean'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 48 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed: 30.8min\n",
      "[Parallel(n_jobs=4)]: Done 480 out of 480 | elapsed: 34.0min finished\n",
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -14.53073, std: 1.19704, params: {'max_depth': 14, 'num_leaves': 50},\n",
       "  mean: -14.74203, std: 1.15113, params: {'max_depth': 14, 'num_leaves': 80},\n",
       "  mean: -15.06348, std: 1.24828, params: {'max_depth': 14, 'num_leaves': 110},\n",
       "  mean: -15.27329, std: 1.18577, params: {'max_depth': 14, 'num_leaves': 140},\n",
       "  mean: -15.66745, std: 1.27811, params: {'max_depth': 14, 'num_leaves': 170},\n",
       "  mean: -15.89216, std: 1.29410, params: {'max_depth': 14, 'num_leaves': 200},\n",
       "  mean: -14.53524, std: 1.22472, params: {'max_depth': 16, 'num_leaves': 50},\n",
       "  mean: -14.65723, std: 1.20361, params: {'max_depth': 16, 'num_leaves': 80},\n",
       "  mean: -14.96363, std: 1.24170, params: {'max_depth': 16, 'num_leaves': 110},\n",
       "  mean: -15.24224, std: 1.19603, params: {'max_depth': 16, 'num_leaves': 140},\n",
       "  mean: -15.63958, std: 1.21606, params: {'max_depth': 16, 'num_leaves': 170},\n",
       "  mean: -15.87957, std: 1.26570, params: {'max_depth': 16, 'num_leaves': 200},\n",
       "  mean: -14.59488, std: 1.27731, params: {'max_depth': 18, 'num_leaves': 50},\n",
       "  mean: -14.66070, std: 1.22848, params: {'max_depth': 18, 'num_leaves': 80},\n",
       "  mean: -14.95786, std: 1.24630, params: {'max_depth': 18, 'num_leaves': 110},\n",
       "  mean: -15.28835, std: 1.21353, params: {'max_depth': 18, 'num_leaves': 140},\n",
       "  mean: -15.51518, std: 1.18520, params: {'max_depth': 18, 'num_leaves': 170},\n",
       "  mean: -15.81086, std: 1.20275, params: {'max_depth': 18, 'num_leaves': 200},\n",
       "  mean: -14.57573, std: 1.29362, params: {'max_depth': 20, 'num_leaves': 50},\n",
       "  mean: -14.72381, std: 1.28247, params: {'max_depth': 20, 'num_leaves': 80},\n",
       "  mean: -14.91038, std: 1.26802, params: {'max_depth': 20, 'num_leaves': 110},\n",
       "  mean: -15.22702, std: 1.22837, params: {'max_depth': 20, 'num_leaves': 140},\n",
       "  mean: -15.55624, std: 1.29394, params: {'max_depth': 20, 'num_leaves': 170},\n",
       "  mean: -15.87846, std: 1.15123, params: {'max_depth': 20, 'num_leaves': 200},\n",
       "  mean: -14.59160, std: 1.30446, params: {'max_depth': 22, 'num_leaves': 50},\n",
       "  mean: -14.66117, std: 1.22154, params: {'max_depth': 22, 'num_leaves': 80},\n",
       "  mean: -14.90481, std: 1.31280, params: {'max_depth': 22, 'num_leaves': 110},\n",
       "  mean: -15.29613, std: 1.23588, params: {'max_depth': 22, 'num_leaves': 140},\n",
       "  mean: -15.56642, std: 1.37442, params: {'max_depth': 22, 'num_leaves': 170},\n",
       "  mean: -15.81546, std: 1.24429, params: {'max_depth': 22, 'num_leaves': 200},\n",
       "  mean: -14.59504, std: 1.30299, params: {'max_depth': 24, 'num_leaves': 50},\n",
       "  mean: -14.67421, std: 1.21874, params: {'max_depth': 24, 'num_leaves': 80},\n",
       "  mean: -14.97998, std: 1.26405, params: {'max_depth': 24, 'num_leaves': 110},\n",
       "  mean: -15.27482, std: 1.16209, params: {'max_depth': 24, 'num_leaves': 140},\n",
       "  mean: -15.54186, std: 1.33362, params: {'max_depth': 24, 'num_leaves': 170},\n",
       "  mean: -15.87327, std: 1.28829, params: {'max_depth': 24, 'num_leaves': 200},\n",
       "  mean: -14.59455, std: 1.30320, params: {'max_depth': 26, 'num_leaves': 50},\n",
       "  mean: -14.65827, std: 1.23052, params: {'max_depth': 26, 'num_leaves': 80},\n",
       "  mean: -14.89966, std: 1.32253, params: {'max_depth': 26, 'num_leaves': 110},\n",
       "  mean: -15.23585, std: 1.13323, params: {'max_depth': 26, 'num_leaves': 140},\n",
       "  mean: -15.61656, std: 1.38340, params: {'max_depth': 26, 'num_leaves': 170},\n",
       "  mean: -15.80009, std: 1.26762, params: {'max_depth': 26, 'num_leaves': 200},\n",
       "  mean: -14.59455, std: 1.30320, params: {'max_depth': 28, 'num_leaves': 50},\n",
       "  mean: -14.65827, std: 1.23052, params: {'max_depth': 28, 'num_leaves': 80},\n",
       "  mean: -14.94162, std: 1.30928, params: {'max_depth': 28, 'num_leaves': 110},\n",
       "  mean: -15.19303, std: 1.20542, params: {'max_depth': 28, 'num_leaves': 140},\n",
       "  mean: -15.60538, std: 1.31622, params: {'max_depth': 28, 'num_leaves': 170},\n",
       "  mean: -15.83709, std: 1.29846, params: {'max_depth': 28, 'num_leaves': 200}],\n",
       " {'max_depth': 14, 'num_leaves': 50},\n",
       " -14.53072824844755)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test1={\n",
    "    'max_depth': range(14,30,2),\n",
    "    'num_leaves':range(50, 230, 30)\n",
    "}\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=991, max_depth=17,\n",
    "                              metric='neg_mean_squared_error', bagging_fraction = 0.8,feature_fraction = 0.8)\n",
    "\n",
    "\n",
    "gsearch1 = GridSearchCV(estimator=model_lgb, param_grid=params_test1, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "gsearch1.fit(X_train, y_train)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "\n",
    "#{'max_depth': 11, 'num_leaves': 50},-14.61332354663911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 40 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=4)]: Done 400 out of 400 | elapsed: 18.4min finished\n",
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -15.15084, std: 1.26589, params: {'max_depth': 9, 'num_leaves': 30},\n",
       "  mean: -14.81834, std: 1.19774, params: {'max_depth': 9, 'num_leaves': 50},\n",
       "  mean: -14.87614, std: 1.18962, params: {'max_depth': 9, 'num_leaves': 70},\n",
       "  mean: -15.18700, std: 1.22639, params: {'max_depth': 9, 'num_leaves': 110},\n",
       "  mean: -15.32749, std: 1.20843, params: {'max_depth': 9, 'num_leaves': 150},\n",
       "  mean: -14.92469, std: 1.39371, params: {'max_depth': 11, 'num_leaves': 30},\n",
       "  mean: -14.63806, std: 1.17701, params: {'max_depth': 11, 'num_leaves': 50},\n",
       "  mean: -14.72325, std: 1.23076, params: {'max_depth': 11, 'num_leaves': 70},\n",
       "  mean: -15.11006, std: 1.32815, params: {'max_depth': 11, 'num_leaves': 110},\n",
       "  mean: -15.47834, std: 1.26172, params: {'max_depth': 11, 'num_leaves': 150},\n",
       "  mean: -14.94421, std: 1.29532, params: {'max_depth': 13, 'num_leaves': 30},\n",
       "  mean: -14.64094, std: 1.18382, params: {'max_depth': 13, 'num_leaves': 50},\n",
       "  mean: -14.66776, std: 1.26604, params: {'max_depth': 13, 'num_leaves': 70},\n",
       "  mean: -14.99477, std: 1.27409, params: {'max_depth': 13, 'num_leaves': 110},\n",
       "  mean: -15.54528, std: 1.22933, params: {'max_depth': 13, 'num_leaves': 150},\n",
       "  mean: -14.89863, std: 1.31845, params: {'max_depth': 15, 'num_leaves': 30},\n",
       "  mean: -14.64081, std: 1.25471, params: {'max_depth': 15, 'num_leaves': 50},\n",
       "  mean: -14.66157, std: 1.25611, params: {'max_depth': 15, 'num_leaves': 70},\n",
       "  mean: -15.01997, std: 1.30354, params: {'max_depth': 15, 'num_leaves': 110},\n",
       "  mean: -15.36546, std: 1.17325, params: {'max_depth': 15, 'num_leaves': 150},\n",
       "  mean: -14.88692, std: 1.26596, params: {'max_depth': 17, 'num_leaves': 30},\n",
       "  mean: -14.50948, std: 1.24941, params: {'max_depth': 17, 'num_leaves': 50},\n",
       "  mean: -14.58195, std: 1.21312, params: {'max_depth': 17, 'num_leaves': 70},\n",
       "  mean: -14.88386, std: 1.24684, params: {'max_depth': 17, 'num_leaves': 110},\n",
       "  mean: -15.41589, std: 1.20481, params: {'max_depth': 17, 'num_leaves': 150},\n",
       "  mean: -14.89069, std: 1.26769, params: {'max_depth': 19, 'num_leaves': 30},\n",
       "  mean: -14.56563, std: 1.31314, params: {'max_depth': 19, 'num_leaves': 50},\n",
       "  mean: -14.60980, std: 1.15891, params: {'max_depth': 19, 'num_leaves': 70},\n",
       "  mean: -14.90222, std: 1.28236, params: {'max_depth': 19, 'num_leaves': 110},\n",
       "  mean: -15.32780, std: 1.23010, params: {'max_depth': 19, 'num_leaves': 150},\n",
       "  mean: -14.89069, std: 1.26769, params: {'max_depth': 21, 'num_leaves': 30},\n",
       "  mean: -14.59393, std: 1.30134, params: {'max_depth': 21, 'num_leaves': 50},\n",
       "  mean: -14.62799, std: 1.19376, params: {'max_depth': 21, 'num_leaves': 70},\n",
       "  mean: -14.95403, std: 1.31673, params: {'max_depth': 21, 'num_leaves': 110},\n",
       "  mean: -15.42561, std: 1.25612, params: {'max_depth': 21, 'num_leaves': 150},\n",
       "  mean: -14.89069, std: 1.26769, params: {'max_depth': 23, 'num_leaves': 30},\n",
       "  mean: -14.59073, std: 1.30485, params: {'max_depth': 23, 'num_leaves': 50},\n",
       "  mean: -14.62924, std: 1.18000, params: {'max_depth': 23, 'num_leaves': 70},\n",
       "  mean: -14.96099, std: 1.27356, params: {'max_depth': 23, 'num_leaves': 110},\n",
       "  mean: -15.33132, std: 1.19328, params: {'max_depth': 23, 'num_leaves': 150}],\n",
       " {'max_depth': 17, 'num_leaves': 50},\n",
       " -14.50948210250792)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test2={\n",
    "    'max_depth': [9,11,13,15,17,19,21,23],\n",
    "    'num_leaves':[30,50,70,110,150]\n",
    "}\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=991, max_depth=14, \n",
    "                              metric='neg_mean_squared_error', bagging_fraction = 0.8, feature_fraction = 0.8)\n",
    "\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator=model_lgb, param_grid=params_test2, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "gsearch2.fit(X_train, y_train)\n",
    "gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  3.2min finished\n",
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -14.40757, std: 1.21543, params: {'min_child_samples': 18, 'min_child_weight': 0.001},\n",
       "  mean: -14.40757, std: 1.21543, params: {'min_child_samples': 18, 'min_child_weight': 0.002},\n",
       "  mean: -14.51272, std: 1.23171, params: {'min_child_samples': 19, 'min_child_weight': 0.001},\n",
       "  mean: -14.51272, std: 1.23171, params: {'min_child_samples': 19, 'min_child_weight': 0.002},\n",
       "  mean: -14.50948, std: 1.24941, params: {'min_child_samples': 20, 'min_child_weight': 0.001},\n",
       "  mean: -14.50948, std: 1.24941, params: {'min_child_samples': 20, 'min_child_weight': 0.002},\n",
       "  mean: -14.62122, std: 1.33035, params: {'min_child_samples': 21, 'min_child_weight': 0.001},\n",
       "  mean: -14.62122, std: 1.33035, params: {'min_child_samples': 21, 'min_child_weight': 0.002},\n",
       "  mean: -14.62197, std: 1.24161, params: {'min_child_samples': 22, 'min_child_weight': 0.001},\n",
       "  mean: -14.62197, std: 1.24161, params: {'min_child_samples': 22, 'min_child_weight': 0.002}],\n",
       " {'min_child_samples': 18, 'min_child_weight': 0.001},\n",
       " -14.407574206002023)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test3={\n",
    "    'min_child_samples': [18, 19, 20, 21, 22],\n",
    "    'min_child_weight':[0.001, 0.002]\n",
    "}\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=991, max_depth=17, \n",
    "                              metric='neg_mean_squared_error', bagging_fraction = 0.8, feature_fraction = 0.8)\n",
    "\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator=model_lgb, param_grid=params_test3, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "gsearch3.fit(X_train, y_train)\n",
    "gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_\n",
    "#{'min_child_samples': 20, 'min_child_weight': 0.001}, -15.94139413891246)\n",
    "# {'min_child_samples': 18, 'min_child_weight': 0.001},-14.996485526598791)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 25 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  7.1min\n",
      "[Parallel(n_jobs=4)]: Done 250 out of 250 | elapsed:  9.0min finished\n",
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -15.95353, std: 1.33075, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.5},\n",
       "  mean: -15.41354, std: 1.33284, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.6},\n",
       "  mean: -15.31771, std: 1.29155, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.7},\n",
       "  mean: -15.46244, std: 1.32509, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.8},\n",
       "  mean: -15.61412, std: 1.48854, params: {'bagging_fraction': 0.6, 'feature_fraction': 0.9},\n",
       "  mean: -15.53017, std: 1.18082, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.5},\n",
       "  mean: -15.10075, std: 1.21634, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.6},\n",
       "  mean: -15.08403, std: 1.25208, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.7},\n",
       "  mean: -15.17861, std: 1.24685, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.8},\n",
       "  mean: -15.23642, std: 1.40932, params: {'bagging_fraction': 0.7, 'feature_fraction': 0.9},\n",
       "  mean: -15.17903, std: 1.17913, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.5},\n",
       "  mean: -14.87179, std: 1.22202, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.6},\n",
       "  mean: -14.73184, std: 1.21952, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.7},\n",
       "  mean: -14.70556, std: 1.24926, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.8},\n",
       "  mean: -14.90618, std: 1.32517, params: {'bagging_fraction': 0.8, 'feature_fraction': 0.9},\n",
       "  mean: -15.01789, std: 1.27528, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.5},\n",
       "  mean: -14.67846, std: 1.27683, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.6},\n",
       "  mean: -14.48877, std: 1.18819, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.7},\n",
       "  mean: -14.46543, std: 1.27293, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.8},\n",
       "  mean: -14.74651, std: 1.27916, params: {'bagging_fraction': 0.9, 'feature_fraction': 0.9},\n",
       "  mean: -15.05256, std: 1.25816, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.5},\n",
       "  mean: -14.68692, std: 1.24270, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.6},\n",
       "  mean: -14.49807, std: 1.15744, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.7},\n",
       "  mean: -14.40757, std: 1.21543, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.8},\n",
       "  mean: -14.61204, std: 1.34950, params: {'bagging_fraction': 1.0, 'feature_fraction': 0.9}],\n",
       " {'bagging_fraction': 1.0, 'feature_fraction': 0.8},\n",
       " -14.407574206002023)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test4={\n",
    "    'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=991, max_depth=17, \n",
    "                              metric='neg_mean_squared_error', bagging_freq = 5,  min_child_samples=18)\n",
    "\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator=model_lgb, param_grid=params_test4, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "gsearch4.fit(X_train, y_train)\n",
    "gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_\n",
    "#{'bagging_fraction': 1.0, 'feature_fraction': 0.8}, -15.94139413891246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:  1.5min finished\n",
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -14.40757, std: 1.21543, params: {'feature_fraction': 0.78},\n",
       "  mean: -14.40757, std: 1.21543, params: {'feature_fraction': 0.79},\n",
       "  mean: -14.40757, std: 1.21543, params: {'feature_fraction': 0.8},\n",
       "  mean: -14.62531, std: 1.30434, params: {'feature_fraction': 0.81},\n",
       "  mean: -14.62531, std: 1.30434, params: {'feature_fraction': 0.82}],\n",
       " {'feature_fraction': 0.78},\n",
       " -14.407574206002023)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_test5={\n",
    "    'feature_fraction': [ 0.78, 0.79, 0.80,0.81,0.82 ]\n",
    "}\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=991, max_depth=17, bagging_fraction = 1.0, \n",
    "                              metric='neg_mean_squared_error', bagging_freq = 5,  min_child_samples=18)\n",
    "\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator=model_lgb, param_grid=params_test5, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "gsearch5.fit(X_train, y_train)\n",
    "gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_\n",
    "#{'feature_fraction': 0.78},-15.94139413891246)\n",
    "# {'feature_fraction': 0.77},-15.94139413891246)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 49 candidates, totalling 490 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=4)]: Done 490 out of 490 | elapsed: 16.0min finished\n",
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:762: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: -14.40757, std: 1.21543, params: {'reg_alpha': 0, 'reg_lambda': 0},\n",
       "  mean: -14.47889, std: 1.26984, params: {'reg_alpha': 0, 'reg_lambda': 0.001},\n",
       "  mean: -14.52121, std: 1.28101, params: {'reg_alpha': 0, 'reg_lambda': 0.01},\n",
       "  mean: -14.52836, std: 1.28210, params: {'reg_alpha': 0, 'reg_lambda': 0.03},\n",
       "  mean: -14.50836, std: 1.17809, params: {'reg_alpha': 0, 'reg_lambda': 0.08},\n",
       "  mean: -14.55525, std: 1.25853, params: {'reg_alpha': 0, 'reg_lambda': 0.3},\n",
       "  mean: -14.57372, std: 1.36302, params: {'reg_alpha': 0, 'reg_lambda': 0.5},\n",
       "  mean: -14.49904, std: 1.29032, params: {'reg_alpha': 0.001, 'reg_lambda': 0},\n",
       "  mean: -14.48399, std: 1.29077, params: {'reg_alpha': 0.001, 'reg_lambda': 0.001},\n",
       "  mean: -14.55460, std: 1.22174, params: {'reg_alpha': 0.001, 'reg_lambda': 0.01},\n",
       "  mean: -14.58423, std: 1.24691, params: {'reg_alpha': 0.001, 'reg_lambda': 0.03},\n",
       "  mean: -14.49225, std: 1.23125, params: {'reg_alpha': 0.001, 'reg_lambda': 0.08},\n",
       "  mean: -14.49639, std: 1.18859, params: {'reg_alpha': 0.001, 'reg_lambda': 0.3},\n",
       "  mean: -14.53938, std: 1.26259, params: {'reg_alpha': 0.001, 'reg_lambda': 0.5},\n",
       "  mean: -14.58609, std: 1.25390, params: {'reg_alpha': 0.01, 'reg_lambda': 0},\n",
       "  mean: -14.54776, std: 1.23387, params: {'reg_alpha': 0.01, 'reg_lambda': 0.001},\n",
       "  mean: -14.48152, std: 1.20448, params: {'reg_alpha': 0.01, 'reg_lambda': 0.01},\n",
       "  mean: -14.53820, std: 1.28200, params: {'reg_alpha': 0.01, 'reg_lambda': 0.03},\n",
       "  mean: -14.53571, std: 1.25754, params: {'reg_alpha': 0.01, 'reg_lambda': 0.08},\n",
       "  mean: -14.56657, std: 1.22226, params: {'reg_alpha': 0.01, 'reg_lambda': 0.3},\n",
       "  mean: -14.64339, std: 1.19767, params: {'reg_alpha': 0.01, 'reg_lambda': 0.5},\n",
       "  mean: -14.57265, std: 1.20034, params: {'reg_alpha': 0.03, 'reg_lambda': 0},\n",
       "  mean: -14.62468, std: 1.14625, params: {'reg_alpha': 0.03, 'reg_lambda': 0.001},\n",
       "  mean: -14.49170, std: 1.20732, params: {'reg_alpha': 0.03, 'reg_lambda': 0.01},\n",
       "  mean: -14.69526, std: 1.17452, params: {'reg_alpha': 0.03, 'reg_lambda': 0.03},\n",
       "  mean: -14.48827, std: 1.27957, params: {'reg_alpha': 0.03, 'reg_lambda': 0.08},\n",
       "  mean: -14.70247, std: 1.20763, params: {'reg_alpha': 0.03, 'reg_lambda': 0.3},\n",
       "  mean: -14.53295, std: 1.26202, params: {'reg_alpha': 0.03, 'reg_lambda': 0.5},\n",
       "  mean: -14.64335, std: 1.25129, params: {'reg_alpha': 0.08, 'reg_lambda': 0},\n",
       "  mean: -14.57090, std: 1.20511, params: {'reg_alpha': 0.08, 'reg_lambda': 0.001},\n",
       "  mean: -14.71308, std: 1.23282, params: {'reg_alpha': 0.08, 'reg_lambda': 0.01},\n",
       "  mean: -14.66166, std: 1.26957, params: {'reg_alpha': 0.08, 'reg_lambda': 0.03},\n",
       "  mean: -14.64615, std: 1.27566, params: {'reg_alpha': 0.08, 'reg_lambda': 0.08},\n",
       "  mean: -14.61604, std: 1.15057, params: {'reg_alpha': 0.08, 'reg_lambda': 0.3},\n",
       "  mean: -14.64086, std: 1.24380, params: {'reg_alpha': 0.08, 'reg_lambda': 0.5},\n",
       "  mean: -14.72477, std: 1.31495, params: {'reg_alpha': 0.3, 'reg_lambda': 0},\n",
       "  mean: -14.79312, std: 1.31294, params: {'reg_alpha': 0.3, 'reg_lambda': 0.001},\n",
       "  mean: -14.72366, std: 1.17552, params: {'reg_alpha': 0.3, 'reg_lambda': 0.01},\n",
       "  mean: -14.78918, std: 1.28574, params: {'reg_alpha': 0.3, 'reg_lambda': 0.03},\n",
       "  mean: -14.74631, std: 1.19220, params: {'reg_alpha': 0.3, 'reg_lambda': 0.08},\n",
       "  mean: -14.70319, std: 1.26898, params: {'reg_alpha': 0.3, 'reg_lambda': 0.3},\n",
       "  mean: -14.70868, std: 1.25215, params: {'reg_alpha': 0.3, 'reg_lambda': 0.5},\n",
       "  mean: -14.77787, std: 1.25235, params: {'reg_alpha': 0.5, 'reg_lambda': 0},\n",
       "  mean: -14.70490, std: 1.18783, params: {'reg_alpha': 0.5, 'reg_lambda': 0.001},\n",
       "  mean: -14.71757, std: 1.17161, params: {'reg_alpha': 0.5, 'reg_lambda': 0.01},\n",
       "  mean: -14.80112, std: 1.20103, params: {'reg_alpha': 0.5, 'reg_lambda': 0.03},\n",
       "  mean: -14.76149, std: 1.32660, params: {'reg_alpha': 0.5, 'reg_lambda': 0.08},\n",
       "  mean: -14.69978, std: 1.22309, params: {'reg_alpha': 0.5, 'reg_lambda': 0.3},\n",
       "  mean: -14.75850, std: 1.23138, params: {'reg_alpha': 0.5, 'reg_lambda': 0.5}],\n",
       " {'reg_alpha': 0, 'reg_lambda': 0},\n",
       " -14.407574206002023)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step5: regularization\n",
    "params_test6={\n",
    "    'reg_alpha': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5],\n",
    "    'reg_lambda': [0, 0.001, 0.01, 0.03, 0.08, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=50,\n",
    "                              learning_rate=0.1, n_estimators=991, max_depth=17, bagging_fraction = 1.0, \n",
    "                              metric='neg_mean_squared_error', bagging_freq = 5,  min_child_samples=18, feature_fraction=0.78)\n",
    "\n",
    "\n",
    "gsearch6 = GridSearchCV(estimator=model_lgb, param_grid=params_test6, scoring='neg_mean_squared_error', cv=10, verbose=1, n_jobs=4)\n",
    "gsearch6.fit(X_train, y_train)\n",
    "gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WANGJIANMING\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:530: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000438 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Unknown parameter: min_child_sample\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000358 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Unknown parameter: min_child_sample\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000299 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Unknown parameter: min_child_sample\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Unknown parameter: min_child_sample\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000308 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 110\n",
      "[LightGBM] [Info] Number of data points in the train set: 8960, number of used features: 18\n",
      "[LightGBM] [Warning] Unknown parameter: min_child_sample\n",
      "[100]\tcv_agg's rmse: 10.7825 + 0.0492403\n",
      "[200]\tcv_agg's rmse: 8.83717 + 0.0522482\n",
      "[300]\tcv_agg's rmse: 7.50058 + 0.0598039\n",
      "[400]\tcv_agg's rmse: 6.61742 + 0.0710896\n",
      "[500]\tcv_agg's rmse: 6.04578 + 0.0856434\n",
      "[600]\tcv_agg's rmse: 5.66381 + 0.0989621\n",
      "[700]\tcv_agg's rmse: 5.40268 + 0.109782\n",
      "[800]\tcv_agg's rmse: 5.22132 + 0.119577\n",
      "[900]\tcv_agg's rmse: 5.09353 + 0.126665\n",
      "best n_estimators: 991\n",
      "best cv score: 5.0115762162612185\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt', \n",
    "    'objective': 'regression', \n",
    "\n",
    "    'learning_rate': 0.003, \n",
    "    'num_leaves': 50, \n",
    "    'max_depth': 17,\n",
    "    'n_estimators':991,\n",
    "    'bagging_fraction':1.0,\n",
    "    'min_child_sample':18,\n",
    "    'feature_fraction':0.78 \n",
    "    }\n",
    "\n",
    "data_train = lgb.Dataset(X_train, y_train, silent=True)\n",
    "cv_results = lgb.cv(\n",
    "    params, data_train, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='rmse',\n",
    "    early_stopping_rounds=50, verbose_eval=100, show_stdv=True)\n",
    "\n",
    "print('best n_estimators:', len(cv_results['rmse-mean']))\n",
    "print('best cv score:', cv_results['rmse-mean'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM--Step by Step to Use GridSearch--Prediction and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.78, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.78\n",
      "[LightGBM] [Warning] bagging_fraction is set=1.0, subsample=1.0 will be ignored. Current value: bagging_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\n",
      "[1]\tvalid_0's rmse: 13.5273\n",
      "Training until validation scores don't improve for 1000 rounds\n",
      "[2]\tvalid_0's rmse: 13.4764\n",
      "[3]\tvalid_0's rmse: 13.4208\n",
      "[4]\tvalid_0's rmse: 13.3659\n",
      "[5]\tvalid_0's rmse: 13.311\n",
      "[6]\tvalid_0's rmse: 13.2565\n",
      "[7]\tvalid_0's rmse: 13.2019\n",
      "[8]\tvalid_0's rmse: 13.1476\n",
      "[9]\tvalid_0's rmse: 13.0941\n",
      "[10]\tvalid_0's rmse: 13.045\n",
      "[11]\tvalid_0's rmse: 12.9915\n",
      "[12]\tvalid_0's rmse: 12.9383\n",
      "[13]\tvalid_0's rmse: 12.8855\n",
      "[14]\tvalid_0's rmse: 12.8335\n",
      "[15]\tvalid_0's rmse: 12.7818\n",
      "[16]\tvalid_0's rmse: 12.7299\n",
      "[17]\tvalid_0's rmse: 12.6787\n",
      "[18]\tvalid_0's rmse: 12.6315\n",
      "[19]\tvalid_0's rmse: 12.5853\n",
      "[20]\tvalid_0's rmse: 12.5351\n",
      "[21]\tvalid_0's rmse: 12.4849\n",
      "[22]\tvalid_0's rmse: 12.4354\n",
      "[23]\tvalid_0's rmse: 12.3893\n",
      "[24]\tvalid_0's rmse: 12.3401\n",
      "[25]\tvalid_0's rmse: 12.2952\n",
      "[26]\tvalid_0's rmse: 12.2467\n",
      "[27]\tvalid_0's rmse: 12.1978\n",
      "[28]\tvalid_0's rmse: 12.1495\n",
      "[29]\tvalid_0's rmse: 12.1018\n",
      "[30]\tvalid_0's rmse: 12.0541\n",
      "[31]\tvalid_0's rmse: 12.0104\n",
      "[32]\tvalid_0's rmse: 11.9632\n",
      "[33]\tvalid_0's rmse: 11.9201\n",
      "[34]\tvalid_0's rmse: 11.8731\n",
      "[35]\tvalid_0's rmse: 11.8266\n",
      "[36]\tvalid_0's rmse: 11.7846\n",
      "[37]\tvalid_0's rmse: 11.739\n",
      "[38]\tvalid_0's rmse: 11.6932\n",
      "[39]\tvalid_0's rmse: 11.6477\n",
      "[40]\tvalid_0's rmse: 11.6071\n",
      "[41]\tvalid_0's rmse: 11.5622\n",
      "[42]\tvalid_0's rmse: 11.5174\n",
      "[43]\tvalid_0's rmse: 11.4738\n",
      "[44]\tvalid_0's rmse: 11.4302\n",
      "[45]\tvalid_0's rmse: 11.3867\n",
      "[46]\tvalid_0's rmse: 11.3461\n",
      "[47]\tvalid_0's rmse: 11.303\n",
      "[48]\tvalid_0's rmse: 11.2598\n",
      "[49]\tvalid_0's rmse: 11.2171\n",
      "[50]\tvalid_0's rmse: 11.1792\n",
      "[51]\tvalid_0's rmse: 11.1368\n",
      "[52]\tvalid_0's rmse: 11.0981\n",
      "[53]\tvalid_0's rmse: 11.0565\n",
      "[54]\tvalid_0's rmse: 11.0147\n",
      "[55]\tvalid_0's rmse: 10.9763\n",
      "[56]\tvalid_0's rmse: 10.9354\n",
      "[57]\tvalid_0's rmse: 10.8974\n",
      "[58]\tvalid_0's rmse: 10.8569\n",
      "[59]\tvalid_0's rmse: 10.8164\n",
      "[60]\tvalid_0's rmse: 10.776\n",
      "[61]\tvalid_0's rmse: 10.736\n",
      "[62]\tvalid_0's rmse: 10.7003\n",
      "[63]\tvalid_0's rmse: 10.6608\n",
      "[64]\tvalid_0's rmse: 10.6243\n",
      "[65]\tvalid_0's rmse: 10.5858\n",
      "[66]\tvalid_0's rmse: 10.5473\n",
      "[67]\tvalid_0's rmse: 10.5119\n",
      "[68]\tvalid_0's rmse: 10.4771\n",
      "[69]\tvalid_0's rmse: 10.4426\n",
      "[70]\tvalid_0's rmse: 10.4046\n",
      "[71]\tvalid_0's rmse: 10.3669\n",
      "[72]\tvalid_0's rmse: 10.3322\n",
      "[73]\tvalid_0's rmse: 10.298\n",
      "[74]\tvalid_0's rmse: 10.261\n",
      "[75]\tvalid_0's rmse: 10.2244\n",
      "[76]\tvalid_0's rmse: 10.1877\n",
      "[77]\tvalid_0's rmse: 10.1519\n",
      "[78]\tvalid_0's rmse: 10.1164\n",
      "[79]\tvalid_0's rmse: 10.0807\n",
      "[80]\tvalid_0's rmse: 10.0449\n",
      "[81]\tvalid_0's rmse: 10.0093\n",
      "[82]\tvalid_0's rmse: 9.97402\n",
      "[83]\tvalid_0's rmse: 9.93924\n",
      "[84]\tvalid_0's rmse: 9.90478\n",
      "[85]\tvalid_0's rmse: 9.87266\n",
      "[86]\tvalid_0's rmse: 9.84042\n",
      "[87]\tvalid_0's rmse: 9.80619\n",
      "[88]\tvalid_0's rmse: 9.77211\n",
      "[89]\tvalid_0's rmse: 9.73832\n",
      "[90]\tvalid_0's rmse: 9.70496\n",
      "[91]\tvalid_0's rmse: 9.67233\n",
      "[92]\tvalid_0's rmse: 9.64153\n",
      "[93]\tvalid_0's rmse: 9.60879\n",
      "[94]\tvalid_0's rmse: 9.57634\n",
      "[95]\tvalid_0's rmse: 9.54348\n",
      "[96]\tvalid_0's rmse: 9.51345\n",
      "[97]\tvalid_0's rmse: 9.48159\n",
      "[98]\tvalid_0's rmse: 9.44931\n",
      "[99]\tvalid_0's rmse: 9.42037\n",
      "[100]\tvalid_0's rmse: 9.39238\n",
      "[101]\tvalid_0's rmse: 9.36102\n",
      "[102]\tvalid_0's rmse: 9.33039\n",
      "[103]\tvalid_0's rmse: 9.29948\n",
      "[104]\tvalid_0's rmse: 9.26855\n",
      "[105]\tvalid_0's rmse: 9.23992\n",
      "[106]\tvalid_0's rmse: 9.20939\n",
      "[107]\tvalid_0's rmse: 9.17991\n",
      "[108]\tvalid_0's rmse: 9.14984\n",
      "[109]\tvalid_0's rmse: 9.12028\n",
      "[110]\tvalid_0's rmse: 9.0905\n",
      "[111]\tvalid_0's rmse: 9.06358\n",
      "[112]\tvalid_0's rmse: 9.03592\n",
      "[113]\tvalid_0's rmse: 9.00743\n",
      "[114]\tvalid_0's rmse: 8.97862\n",
      "[115]\tvalid_0's rmse: 8.94975\n",
      "[116]\tvalid_0's rmse: 8.92137\n",
      "[117]\tvalid_0's rmse: 8.89298\n",
      "[118]\tvalid_0's rmse: 8.86775\n",
      "[119]\tvalid_0's rmse: 8.8399\n",
      "[120]\tvalid_0's rmse: 8.81233\n",
      "[121]\tvalid_0's rmse: 8.78446\n",
      "[122]\tvalid_0's rmse: 8.75727\n",
      "[123]\tvalid_0's rmse: 8.72975\n",
      "[124]\tvalid_0's rmse: 8.70283\n",
      "[125]\tvalid_0's rmse: 8.67635\n",
      "[126]\tvalid_0's rmse: 8.65191\n",
      "[127]\tvalid_0's rmse: 8.62782\n",
      "[128]\tvalid_0's rmse: 8.60139\n",
      "[129]\tvalid_0's rmse: 8.57492\n",
      "[130]\tvalid_0's rmse: 8.54875\n",
      "[131]\tvalid_0's rmse: 8.5226\n",
      "[132]\tvalid_0's rmse: 8.49657\n",
      "[133]\tvalid_0's rmse: 8.47091\n",
      "[134]\tvalid_0's rmse: 8.44596\n",
      "[135]\tvalid_0's rmse: 8.44256\n",
      "[136]\tvalid_0's rmse: 8.41751\n",
      "[137]\tvalid_0's rmse: 8.39303\n",
      "[138]\tvalid_0's rmse: 8.36831\n",
      "[139]\tvalid_0's rmse: 8.34382\n",
      "[140]\tvalid_0's rmse: 8.31927\n",
      "[141]\tvalid_0's rmse: 8.29671\n",
      "[142]\tvalid_0's rmse: 8.27236\n",
      "[143]\tvalid_0's rmse: 8.24742\n",
      "[144]\tvalid_0's rmse: 8.22361\n",
      "[145]\tvalid_0's rmse: 8.19965\n",
      "[146]\tvalid_0's rmse: 8.17522\n",
      "[147]\tvalid_0's rmse: 8.1519\n",
      "[148]\tvalid_0's rmse: 8.12906\n",
      "[149]\tvalid_0's rmse: 8.10706\n",
      "[150]\tvalid_0's rmse: 8.08426\n",
      "[151]\tvalid_0's rmse: 8.06209\n",
      "[152]\tvalid_0's rmse: 8.03936\n",
      "[153]\tvalid_0's rmse: 8.01596\n",
      "[154]\tvalid_0's rmse: 7.9952\n",
      "[155]\tvalid_0's rmse: 7.97332\n",
      "[156]\tvalid_0's rmse: 7.95033\n",
      "[157]\tvalid_0's rmse: 7.92992\n",
      "[158]\tvalid_0's rmse: 7.90836\n",
      "[159]\tvalid_0's rmse: 7.88664\n",
      "[160]\tvalid_0's rmse: 7.86507\n",
      "[161]\tvalid_0's rmse: 7.84368\n",
      "[162]\tvalid_0's rmse: 7.82268\n",
      "[163]\tvalid_0's rmse: 7.80165\n",
      "[164]\tvalid_0's rmse: 7.78128\n",
      "[165]\tvalid_0's rmse: 7.76048\n",
      "[166]\tvalid_0's rmse: 7.74186\n",
      "[167]\tvalid_0's rmse: 7.72132\n",
      "[168]\tvalid_0's rmse: 7.70087\n",
      "[169]\tvalid_0's rmse: 7.67994\n",
      "[170]\tvalid_0's rmse: 7.66035\n",
      "[171]\tvalid_0's rmse: 7.6412\n",
      "[172]\tvalid_0's rmse: 7.62149\n",
      "[173]\tvalid_0's rmse: 7.60179\n",
      "[174]\tvalid_0's rmse: 7.58251\n",
      "[175]\tvalid_0's rmse: 7.56461\n",
      "[176]\tvalid_0's rmse: 7.54575\n",
      "[177]\tvalid_0's rmse: 7.52591\n",
      "[178]\tvalid_0's rmse: 7.50737\n",
      "[179]\tvalid_0's rmse: 7.48905\n",
      "[180]\tvalid_0's rmse: 7.47099\n",
      "[181]\tvalid_0's rmse: 7.45233\n",
      "[182]\tvalid_0's rmse: 7.43628\n",
      "[183]\tvalid_0's rmse: 7.41792\n",
      "[184]\tvalid_0's rmse: 7.40035\n",
      "[185]\tvalid_0's rmse: 7.38274\n",
      "[186]\tvalid_0's rmse: 7.36486\n",
      "[187]\tvalid_0's rmse: 7.34714\n",
      "[188]\tvalid_0's rmse: 7.3294\n",
      "[189]\tvalid_0's rmse: 7.3118\n",
      "[190]\tvalid_0's rmse: 7.29416\n",
      "[191]\tvalid_0's rmse: 7.27678\n",
      "[192]\tvalid_0's rmse: 7.25863\n",
      "[193]\tvalid_0's rmse: 7.24155\n",
      "[194]\tvalid_0's rmse: 7.22483\n",
      "[195]\tvalid_0's rmse: 7.20804\n",
      "[196]\tvalid_0's rmse: 7.19252\n",
      "[197]\tvalid_0's rmse: 7.17599\n",
      "[198]\tvalid_0's rmse: 7.1593\n",
      "[199]\tvalid_0's rmse: 7.14317\n",
      "[200]\tvalid_0's rmse: 7.12704\n",
      "[201]\tvalid_0's rmse: 7.11078\n",
      "[202]\tvalid_0's rmse: 7.09485\n",
      "[203]\tvalid_0's rmse: 7.07862\n",
      "[204]\tvalid_0's rmse: 7.06278\n",
      "[205]\tvalid_0's rmse: 7.04756\n",
      "[206]\tvalid_0's rmse: 7.03316\n",
      "[207]\tvalid_0's rmse: 7.01776\n",
      "[208]\tvalid_0's rmse: 7.00247\n",
      "[209]\tvalid_0's rmse: 6.98738\n",
      "[210]\tvalid_0's rmse: 6.9714\n",
      "[211]\tvalid_0's rmse: 6.9563\n",
      "[212]\tvalid_0's rmse: 6.94189\n",
      "[213]\tvalid_0's rmse: 6.92767\n",
      "[214]\tvalid_0's rmse: 6.91346\n",
      "[215]\tvalid_0's rmse: 6.89888\n",
      "[216]\tvalid_0's rmse: 6.8836\n",
      "[217]\tvalid_0's rmse: 6.86945\n",
      "[218]\tvalid_0's rmse: 6.85537\n",
      "[219]\tvalid_0's rmse: 6.84148\n",
      "[220]\tvalid_0's rmse: 6.82756\n",
      "[221]\tvalid_0's rmse: 6.8136\n",
      "[222]\tvalid_0's rmse: 6.79877\n",
      "[223]\tvalid_0's rmse: 6.78488\n",
      "[224]\tvalid_0's rmse: 6.77138\n",
      "[225]\tvalid_0's rmse: 6.75842\n",
      "[226]\tvalid_0's rmse: 6.74564\n",
      "[227]\tvalid_0's rmse: 6.73165\n",
      "[228]\tvalid_0's rmse: 6.71815\n",
      "[229]\tvalid_0's rmse: 6.70501\n",
      "[230]\tvalid_0's rmse: 6.69239\n",
      "[231]\tvalid_0's rmse: 6.67978\n",
      "[232]\tvalid_0's rmse: 6.66621\n",
      "[233]\tvalid_0's rmse: 6.65368\n",
      "[234]\tvalid_0's rmse: 6.64097\n",
      "[235]\tvalid_0's rmse: 6.62825\n",
      "[236]\tvalid_0's rmse: 6.61575\n",
      "[237]\tvalid_0's rmse: 6.60378\n",
      "[238]\tvalid_0's rmse: 6.59053\n",
      "[239]\tvalid_0's rmse: 6.57863\n",
      "[240]\tvalid_0's rmse: 6.5664\n",
      "[241]\tvalid_0's rmse: 6.55343\n",
      "[242]\tvalid_0's rmse: 6.54014\n",
      "[243]\tvalid_0's rmse: 6.52857\n",
      "[244]\tvalid_0's rmse: 6.51538\n",
      "[245]\tvalid_0's rmse: 6.50282\n",
      "[246]\tvalid_0's rmse: 6.49064\n",
      "[247]\tvalid_0's rmse: 6.47911\n",
      "[248]\tvalid_0's rmse: 6.4663\n",
      "[249]\tvalid_0's rmse: 6.45365\n",
      "[250]\tvalid_0's rmse: 6.4427\n",
      "[251]\tvalid_0's rmse: 6.43103\n",
      "[252]\tvalid_0's rmse: 6.42026\n",
      "[253]\tvalid_0's rmse: 6.40927\n",
      "[254]\tvalid_0's rmse: 6.39839\n",
      "[255]\tvalid_0's rmse: 6.38788\n",
      "[256]\tvalid_0's rmse: 6.37725\n",
      "[257]\tvalid_0's rmse: 6.36642\n",
      "[258]\tvalid_0's rmse: 6.35567\n",
      "[259]\tvalid_0's rmse: 6.34499\n",
      "[260]\tvalid_0's rmse: 6.33334\n",
      "[261]\tvalid_0's rmse: 6.32165\n",
      "[262]\tvalid_0's rmse: 6.31168\n",
      "[263]\tvalid_0's rmse: 6.30186\n",
      "[264]\tvalid_0's rmse: 6.29118\n",
      "[265]\tvalid_0's rmse: 6.28075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[266]\tvalid_0's rmse: 6.27081\n",
      "[267]\tvalid_0's rmse: 6.26104\n",
      "[268]\tvalid_0's rmse: 6.2512\n",
      "[269]\tvalid_0's rmse: 6.24164\n",
      "[270]\tvalid_0's rmse: 6.23191\n",
      "[271]\tvalid_0's rmse: 6.22136\n",
      "[272]\tvalid_0's rmse: 6.21179\n",
      "[273]\tvalid_0's rmse: 6.20202\n",
      "[274]\tvalid_0's rmse: 6.19129\n",
      "[275]\tvalid_0's rmse: 6.18183\n",
      "[276]\tvalid_0's rmse: 6.17273\n",
      "[277]\tvalid_0's rmse: 6.16229\n",
      "[278]\tvalid_0's rmse: 6.15305\n",
      "[279]\tvalid_0's rmse: 6.14374\n",
      "[280]\tvalid_0's rmse: 6.13507\n",
      "[281]\tvalid_0's rmse: 6.12529\n",
      "[282]\tvalid_0's rmse: 6.11649\n",
      "[283]\tvalid_0's rmse: 6.10784\n",
      "[284]\tvalid_0's rmse: 6.0991\n",
      "[285]\tvalid_0's rmse: 6.09047\n",
      "[286]\tvalid_0's rmse: 6.08111\n",
      "[287]\tvalid_0's rmse: 6.07251\n",
      "[288]\tvalid_0's rmse: 6.06399\n",
      "[289]\tvalid_0's rmse: 6.05573\n",
      "[290]\tvalid_0's rmse: 6.04714\n",
      "[291]\tvalid_0's rmse: 6.03894\n",
      "[292]\tvalid_0's rmse: 6.03075\n",
      "[293]\tvalid_0's rmse: 6.02257\n",
      "[294]\tvalid_0's rmse: 6.01446\n",
      "[295]\tvalid_0's rmse: 6.00648\n",
      "[296]\tvalid_0's rmse: 5.99847\n",
      "[297]\tvalid_0's rmse: 5.98965\n",
      "[298]\tvalid_0's rmse: 5.98105\n",
      "[299]\tvalid_0's rmse: 5.97297\n",
      "[300]\tvalid_0's rmse: 5.96405\n",
      "[301]\tvalid_0's rmse: 5.95606\n",
      "[302]\tvalid_0's rmse: 5.94807\n",
      "[303]\tvalid_0's rmse: 5.93942\n",
      "[304]\tvalid_0's rmse: 5.93161\n",
      "[305]\tvalid_0's rmse: 5.92433\n",
      "[306]\tvalid_0's rmse: 5.91599\n",
      "[307]\tvalid_0's rmse: 5.90849\n",
      "[308]\tvalid_0's rmse: 5.90138\n",
      "[309]\tvalid_0's rmse: 5.89263\n",
      "[310]\tvalid_0's rmse: 5.88547\n",
      "[311]\tvalid_0's rmse: 5.87793\n",
      "[312]\tvalid_0's rmse: 5.87094\n",
      "[313]\tvalid_0's rmse: 5.86435\n",
      "[314]\tvalid_0's rmse: 5.85636\n",
      "[315]\tvalid_0's rmse: 5.84829\n",
      "[316]\tvalid_0's rmse: 5.84068\n",
      "[317]\tvalid_0's rmse: 5.83395\n",
      "[318]\tvalid_0's rmse: 5.82659\n",
      "[319]\tvalid_0's rmse: 5.81981\n",
      "[320]\tvalid_0's rmse: 5.81249\n",
      "[321]\tvalid_0's rmse: 5.80517\n",
      "[322]\tvalid_0's rmse: 5.79869\n",
      "[323]\tvalid_0's rmse: 5.79225\n",
      "[324]\tvalid_0's rmse: 5.78425\n",
      "[325]\tvalid_0's rmse: 5.77814\n",
      "[326]\tvalid_0's rmse: 5.77177\n",
      "[327]\tvalid_0's rmse: 5.76566\n",
      "[328]\tvalid_0's rmse: 5.75949\n",
      "[329]\tvalid_0's rmse: 5.75224\n",
      "[330]\tvalid_0's rmse: 5.74612\n",
      "[331]\tvalid_0's rmse: 5.7398\n",
      "[332]\tvalid_0's rmse: 5.73225\n",
      "[333]\tvalid_0's rmse: 5.72627\n",
      "[334]\tvalid_0's rmse: 5.72001\n",
      "[335]\tvalid_0's rmse: 5.71427\n",
      "[336]\tvalid_0's rmse: 5.70831\n",
      "[337]\tvalid_0's rmse: 5.70251\n",
      "[338]\tvalid_0's rmse: 5.69679\n",
      "[339]\tvalid_0's rmse: 5.69123\n",
      "[340]\tvalid_0's rmse: 5.68564\n",
      "[341]\tvalid_0's rmse: 5.67953\n",
      "[342]\tvalid_0's rmse: 5.67171\n",
      "[343]\tvalid_0's rmse: 5.6651\n",
      "[344]\tvalid_0's rmse: 5.65839\n",
      "[345]\tvalid_0's rmse: 5.65311\n",
      "[346]\tvalid_0's rmse: 5.64668\n",
      "[347]\tvalid_0's rmse: 5.64134\n",
      "[348]\tvalid_0's rmse: 5.63606\n",
      "[349]\tvalid_0's rmse: 5.63034\n",
      "[350]\tvalid_0's rmse: 5.62392\n",
      "[351]\tvalid_0's rmse: 5.61872\n",
      "[352]\tvalid_0's rmse: 5.61225\n",
      "[353]\tvalid_0's rmse: 5.60708\n",
      "[354]\tvalid_0's rmse: 5.60023\n",
      "[355]\tvalid_0's rmse: 5.59348\n",
      "[356]\tvalid_0's rmse: 5.588\n",
      "[357]\tvalid_0's rmse: 5.5813\n",
      "[358]\tvalid_0's rmse: 5.57472\n",
      "[359]\tvalid_0's rmse: 5.56932\n",
      "[360]\tvalid_0's rmse: 5.56277\n",
      "[361]\tvalid_0's rmse: 5.55657\n",
      "[362]\tvalid_0's rmse: 5.55112\n",
      "[363]\tvalid_0's rmse: 5.54571\n",
      "[364]\tvalid_0's rmse: 5.53938\n",
      "[365]\tvalid_0's rmse: 5.53445\n",
      "[366]\tvalid_0's rmse: 5.52955\n",
      "[367]\tvalid_0's rmse: 5.5244\n",
      "[368]\tvalid_0's rmse: 5.5195\n",
      "[369]\tvalid_0's rmse: 5.51458\n",
      "[370]\tvalid_0's rmse: 5.50999\n",
      "[371]\tvalid_0's rmse: 5.50512\n",
      "[372]\tvalid_0's rmse: 5.49999\n",
      "[373]\tvalid_0's rmse: 5.49526\n",
      "[374]\tvalid_0's rmse: 5.49064\n",
      "[375]\tvalid_0's rmse: 5.48599\n",
      "[376]\tvalid_0's rmse: 5.48121\n",
      "[377]\tvalid_0's rmse: 5.47646\n",
      "[378]\tvalid_0's rmse: 5.4718\n",
      "[379]\tvalid_0's rmse: 5.46734\n",
      "[380]\tvalid_0's rmse: 5.463\n",
      "[381]\tvalid_0's rmse: 5.457\n",
      "[382]\tvalid_0's rmse: 5.45172\n",
      "[383]\tvalid_0's rmse: 5.44733\n",
      "[384]\tvalid_0's rmse: 5.44296\n",
      "[385]\tvalid_0's rmse: 5.43858\n",
      "[386]\tvalid_0's rmse: 5.43398\n",
      "[387]\tvalid_0's rmse: 5.42966\n",
      "[388]\tvalid_0's rmse: 5.42525\n",
      "[389]\tvalid_0's rmse: 5.42128\n",
      "[390]\tvalid_0's rmse: 5.41736\n",
      "[391]\tvalid_0's rmse: 5.41342\n",
      "[392]\tvalid_0's rmse: 5.40878\n",
      "[393]\tvalid_0's rmse: 5.40479\n",
      "[394]\tvalid_0's rmse: 5.40066\n",
      "[395]\tvalid_0's rmse: 5.39597\n",
      "[396]\tvalid_0's rmse: 5.39145\n",
      "[397]\tvalid_0's rmse: 5.38665\n",
      "[398]\tvalid_0's rmse: 5.38269\n",
      "[399]\tvalid_0's rmse: 5.37854\n",
      "[400]\tvalid_0's rmse: 5.37445\n",
      "[401]\tvalid_0's rmse: 5.37017\n",
      "[402]\tvalid_0's rmse: 5.36637\n",
      "[403]\tvalid_0's rmse: 5.36247\n",
      "[404]\tvalid_0's rmse: 5.35874\n",
      "[405]\tvalid_0's rmse: 5.35485\n",
      "[406]\tvalid_0's rmse: 5.35115\n",
      "[407]\tvalid_0's rmse: 5.34695\n",
      "[408]\tvalid_0's rmse: 5.34319\n",
      "[409]\tvalid_0's rmse: 5.33963\n",
      "[410]\tvalid_0's rmse: 5.33584\n",
      "[411]\tvalid_0's rmse: 5.33223\n",
      "[412]\tvalid_0's rmse: 5.32706\n",
      "[413]\tvalid_0's rmse: 5.32316\n",
      "[414]\tvalid_0's rmse: 5.31989\n",
      "[415]\tvalid_0's rmse: 5.31479\n",
      "[416]\tvalid_0's rmse: 5.31049\n",
      "[417]\tvalid_0's rmse: 5.30706\n",
      "[418]\tvalid_0's rmse: 5.30207\n",
      "[419]\tvalid_0's rmse: 5.29884\n",
      "[420]\tvalid_0's rmse: 5.29562\n",
      "[421]\tvalid_0's rmse: 5.29127\n",
      "[422]\tvalid_0's rmse: 5.28789\n",
      "[423]\tvalid_0's rmse: 5.28463\n",
      "[424]\tvalid_0's rmse: 5.28168\n",
      "[425]\tvalid_0's rmse: 5.2786\n",
      "[426]\tvalid_0's rmse: 5.27715\n",
      "[427]\tvalid_0's rmse: 5.27402\n",
      "[428]\tvalid_0's rmse: 5.2711\n",
      "[429]\tvalid_0's rmse: 5.26767\n",
      "[430]\tvalid_0's rmse: 5.26368\n",
      "[431]\tvalid_0's rmse: 5.26042\n",
      "[432]\tvalid_0's rmse: 5.25675\n",
      "[433]\tvalid_0's rmse: 5.25404\n",
      "[434]\tvalid_0's rmse: 5.25076\n",
      "[435]\tvalid_0's rmse: 5.24771\n",
      "[436]\tvalid_0's rmse: 5.24342\n",
      "[437]\tvalid_0's rmse: 5.24029\n",
      "[438]\tvalid_0's rmse: 5.23664\n",
      "[439]\tvalid_0's rmse: 5.23334\n",
      "[440]\tvalid_0's rmse: 5.23037\n",
      "[441]\tvalid_0's rmse: 5.22587\n",
      "[442]\tvalid_0's rmse: 5.22317\n",
      "[443]\tvalid_0's rmse: 5.22031\n",
      "[444]\tvalid_0's rmse: 5.21578\n",
      "[445]\tvalid_0's rmse: 5.21311\n",
      "[446]\tvalid_0's rmse: 5.2091\n",
      "[447]\tvalid_0's rmse: 5.20537\n",
      "[448]\tvalid_0's rmse: 5.20262\n",
      "[449]\tvalid_0's rmse: 5.19982\n",
      "[450]\tvalid_0's rmse: 5.19703\n",
      "[451]\tvalid_0's rmse: 5.19461\n",
      "[452]\tvalid_0's rmse: 5.19093\n",
      "[453]\tvalid_0's rmse: 5.1866\n",
      "[454]\tvalid_0's rmse: 5.18231\n",
      "[455]\tvalid_0's rmse: 5.17805\n",
      "[456]\tvalid_0's rmse: 5.17555\n",
      "[457]\tvalid_0's rmse: 5.17298\n",
      "[458]\tvalid_0's rmse: 5.17047\n",
      "[459]\tvalid_0's rmse: 5.16781\n",
      "[460]\tvalid_0's rmse: 5.16439\n",
      "[461]\tvalid_0's rmse: 5.162\n",
      "[462]\tvalid_0's rmse: 5.15963\n",
      "[463]\tvalid_0's rmse: 5.15654\n",
      "[464]\tvalid_0's rmse: 5.15333\n",
      "[465]\tvalid_0's rmse: 5.15113\n",
      "[466]\tvalid_0's rmse: 5.14855\n",
      "[467]\tvalid_0's rmse: 5.14493\n",
      "[468]\tvalid_0's rmse: 5.1409\n",
      "[469]\tvalid_0's rmse: 5.13735\n",
      "[470]\tvalid_0's rmse: 5.13516\n",
      "[471]\tvalid_0's rmse: 5.13188\n",
      "[472]\tvalid_0's rmse: 5.1284\n",
      "[473]\tvalid_0's rmse: 5.12519\n",
      "[474]\tvalid_0's rmse: 5.12291\n",
      "[475]\tvalid_0's rmse: 5.11901\n",
      "[476]\tvalid_0's rmse: 5.11692\n",
      "[477]\tvalid_0's rmse: 5.11428\n",
      "[478]\tvalid_0's rmse: 5.11043\n",
      "[479]\tvalid_0's rmse: 5.10796\n",
      "[480]\tvalid_0's rmse: 5.10584\n",
      "[481]\tvalid_0's rmse: 5.10368\n",
      "[482]\tvalid_0's rmse: 5.09972\n",
      "[483]\tvalid_0's rmse: 5.0976\n",
      "[484]\tvalid_0's rmse: 5.09515\n",
      "[485]\tvalid_0's rmse: 5.09229\n",
      "[486]\tvalid_0's rmse: 5.09033\n",
      "[487]\tvalid_0's rmse: 5.08725\n",
      "[488]\tvalid_0's rmse: 5.08442\n",
      "[489]\tvalid_0's rmse: 5.08248\n",
      "[490]\tvalid_0's rmse: 5.07994\n",
      "[491]\tvalid_0's rmse: 5.07718\n",
      "[492]\tvalid_0's rmse: 5.07405\n",
      "[493]\tvalid_0's rmse: 5.07121\n",
      "[494]\tvalid_0's rmse: 5.0685\n",
      "[495]\tvalid_0's rmse: 5.06604\n",
      "[496]\tvalid_0's rmse: 5.06362\n",
      "[497]\tvalid_0's rmse: 5.06174\n",
      "[498]\tvalid_0's rmse: 5.05899\n",
      "[499]\tvalid_0's rmse: 5.0565\n",
      "[500]\tvalid_0's rmse: 5.05349\n",
      "[501]\tvalid_0's rmse: 5.05156\n",
      "[502]\tvalid_0's rmse: 5.04967\n",
      "[503]\tvalid_0's rmse: 5.04699\n",
      "[504]\tvalid_0's rmse: 5.04477\n",
      "[505]\tvalid_0's rmse: 5.04245\n",
      "[506]\tvalid_0's rmse: 5.03933\n",
      "[507]\tvalid_0's rmse: 5.03752\n",
      "[508]\tvalid_0's rmse: 5.03511\n",
      "[509]\tvalid_0's rmse: 5.0332\n",
      "[510]\tvalid_0's rmse: 5.03078\n",
      "[511]\tvalid_0's rmse: 5.0279\n",
      "[512]\tvalid_0's rmse: 5.02443\n",
      "[513]\tvalid_0's rmse: 5.02199\n",
      "[514]\tvalid_0's rmse: 5.01896\n",
      "[515]\tvalid_0's rmse: 5.01624\n",
      "[516]\tvalid_0's rmse: 5.01359\n",
      "[517]\tvalid_0's rmse: 5.01122\n",
      "[518]\tvalid_0's rmse: 5.00899\n",
      "[519]\tvalid_0's rmse: 5.00695\n",
      "[520]\tvalid_0's rmse: 5.00498\n",
      "[521]\tvalid_0's rmse: 5.00329\n",
      "[522]\tvalid_0's rmse: 5.00106\n",
      "[523]\tvalid_0's rmse: 4.99876\n",
      "[524]\tvalid_0's rmse: 4.99704\n",
      "[525]\tvalid_0's rmse: 4.99398\n",
      "[526]\tvalid_0's rmse: 4.99246\n",
      "[527]\tvalid_0's rmse: 4.99093\n",
      "[528]\tvalid_0's rmse: 4.98934\n",
      "[529]\tvalid_0's rmse: 4.9877\n",
      "[530]\tvalid_0's rmse: 4.98525\n",
      "[531]\tvalid_0's rmse: 4.98374\n",
      "[532]\tvalid_0's rmse: 4.98176\n",
      "[533]\tvalid_0's rmse: 4.97896\n",
      "[534]\tvalid_0's rmse: 4.97704\n",
      "[535]\tvalid_0's rmse: 4.9755\n",
      "[536]\tvalid_0's rmse: 4.97394\n",
      "[537]\tvalid_0's rmse: 4.97047\n",
      "[538]\tvalid_0's rmse: 4.96898\n",
      "[539]\tvalid_0's rmse: 4.96753\n",
      "[540]\tvalid_0's rmse: 4.96581\n",
      "[541]\tvalid_0's rmse: 4.96437\n",
      "[542]\tvalid_0's rmse: 4.96239\n",
      "[543]\tvalid_0's rmse: 4.9607\n",
      "[544]\tvalid_0's rmse: 4.95875\n",
      "[545]\tvalid_0's rmse: 4.95663\n",
      "[546]\tvalid_0's rmse: 4.95522\n",
      "[547]\tvalid_0's rmse: 4.95308\n",
      "[548]\tvalid_0's rmse: 4.95176\n",
      "[549]\tvalid_0's rmse: 4.95024\n",
      "[550]\tvalid_0's rmse: 4.94811\n",
      "[551]\tvalid_0's rmse: 4.94671\n",
      "[552]\tvalid_0's rmse: 4.94536\n",
      "[553]\tvalid_0's rmse: 4.94398\n",
      "[554]\tvalid_0's rmse: 4.94227\n",
      "[555]\tvalid_0's rmse: 4.94086\n",
      "[556]\tvalid_0's rmse: 4.93924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[557]\tvalid_0's rmse: 4.93808\n",
      "[558]\tvalid_0's rmse: 4.93555\n",
      "[559]\tvalid_0's rmse: 4.93425\n",
      "[560]\tvalid_0's rmse: 4.93142\n",
      "[561]\tvalid_0's rmse: 4.93005\n",
      "[562]\tvalid_0's rmse: 4.9281\n",
      "[563]\tvalid_0's rmse: 4.92687\n",
      "[564]\tvalid_0's rmse: 4.92407\n",
      "[565]\tvalid_0's rmse: 4.92284\n",
      "[566]\tvalid_0's rmse: 4.92088\n",
      "[567]\tvalid_0's rmse: 4.91976\n",
      "[568]\tvalid_0's rmse: 4.91787\n",
      "[569]\tvalid_0's rmse: 4.9147\n",
      "[570]\tvalid_0's rmse: 4.91158\n",
      "[571]\tvalid_0's rmse: 4.91031\n",
      "[572]\tvalid_0's rmse: 4.909\n",
      "[573]\tvalid_0's rmse: 4.90785\n",
      "[574]\tvalid_0's rmse: 4.90614\n",
      "[575]\tvalid_0's rmse: 4.90305\n",
      "[576]\tvalid_0's rmse: 4.90182\n",
      "[577]\tvalid_0's rmse: 4.89876\n",
      "[578]\tvalid_0's rmse: 4.89639\n",
      "[579]\tvalid_0's rmse: 4.8945\n",
      "[580]\tvalid_0's rmse: 4.89289\n",
      "[581]\tvalid_0's rmse: 4.89166\n",
      "[582]\tvalid_0's rmse: 4.89046\n",
      "[583]\tvalid_0's rmse: 4.8875\n",
      "[584]\tvalid_0's rmse: 4.88623\n",
      "[585]\tvalid_0's rmse: 4.88516\n",
      "[586]\tvalid_0's rmse: 4.88283\n",
      "[587]\tvalid_0's rmse: 4.88045\n",
      "[588]\tvalid_0's rmse: 4.87901\n",
      "[589]\tvalid_0's rmse: 4.87795\n",
      "[590]\tvalid_0's rmse: 4.877\n",
      "[591]\tvalid_0's rmse: 4.87595\n",
      "[592]\tvalid_0's rmse: 4.87499\n",
      "[593]\tvalid_0's rmse: 4.87331\n",
      "[594]\tvalid_0's rmse: 4.87222\n",
      "[595]\tvalid_0's rmse: 4.87079\n",
      "[596]\tvalid_0's rmse: 4.86968\n",
      "[597]\tvalid_0's rmse: 4.86864\n",
      "[598]\tvalid_0's rmse: 4.86692\n",
      "[599]\tvalid_0's rmse: 4.86591\n",
      "[600]\tvalid_0's rmse: 4.86494\n",
      "[601]\tvalid_0's rmse: 4.86208\n",
      "[602]\tvalid_0's rmse: 4.85985\n",
      "[603]\tvalid_0's rmse: 4.85892\n",
      "[604]\tvalid_0's rmse: 4.85796\n",
      "[605]\tvalid_0's rmse: 4.8569\n",
      "[606]\tvalid_0's rmse: 4.85464\n",
      "[607]\tvalid_0's rmse: 4.85373\n",
      "[608]\tvalid_0's rmse: 4.85297\n",
      "[609]\tvalid_0's rmse: 4.85156\n",
      "[610]\tvalid_0's rmse: 4.8508\n",
      "[611]\tvalid_0's rmse: 4.84969\n",
      "[612]\tvalid_0's rmse: 4.84689\n",
      "[613]\tvalid_0's rmse: 4.8441\n",
      "[614]\tvalid_0's rmse: 4.84317\n",
      "[615]\tvalid_0's rmse: 4.84075\n",
      "[616]\tvalid_0's rmse: 4.84\n",
      "[617]\tvalid_0's rmse: 4.83791\n",
      "[618]\tvalid_0's rmse: 4.83674\n",
      "[619]\tvalid_0's rmse: 4.83477\n",
      "[620]\tvalid_0's rmse: 4.83395\n",
      "[621]\tvalid_0's rmse: 4.833\n",
      "[622]\tvalid_0's rmse: 4.83205\n",
      "[623]\tvalid_0's rmse: 4.83132\n",
      "[624]\tvalid_0's rmse: 4.83027\n",
      "[625]\tvalid_0's rmse: 4.82926\n",
      "[626]\tvalid_0's rmse: 4.82836\n",
      "[627]\tvalid_0's rmse: 4.82567\n",
      "[628]\tvalid_0's rmse: 4.82357\n",
      "[629]\tvalid_0's rmse: 4.8212\n",
      "[630]\tvalid_0's rmse: 4.8205\n",
      "[631]\tvalid_0's rmse: 4.81789\n",
      "[632]\tvalid_0's rmse: 4.81711\n",
      "[633]\tvalid_0's rmse: 4.81624\n",
      "[634]\tvalid_0's rmse: 4.81535\n",
      "[635]\tvalid_0's rmse: 4.81434\n",
      "[636]\tvalid_0's rmse: 4.81364\n",
      "[637]\tvalid_0's rmse: 4.81289\n",
      "[638]\tvalid_0's rmse: 4.81044\n",
      "[639]\tvalid_0's rmse: 4.8083\n",
      "[640]\tvalid_0's rmse: 4.8077\n",
      "[641]\tvalid_0's rmse: 4.80512\n",
      "[642]\tvalid_0's rmse: 4.80429\n",
      "[643]\tvalid_0's rmse: 4.8035\n",
      "[644]\tvalid_0's rmse: 4.80285\n",
      "[645]\tvalid_0's rmse: 4.80211\n",
      "[646]\tvalid_0's rmse: 4.79974\n",
      "[647]\tvalid_0's rmse: 4.79751\n",
      "[648]\tvalid_0's rmse: 4.79659\n",
      "[649]\tvalid_0's rmse: 4.79571\n",
      "[650]\tvalid_0's rmse: 4.79444\n",
      "[651]\tvalid_0's rmse: 4.79376\n",
      "[652]\tvalid_0's rmse: 4.79302\n",
      "[653]\tvalid_0's rmse: 4.79235\n",
      "[654]\tvalid_0's rmse: 4.79173\n",
      "[655]\tvalid_0's rmse: 4.79023\n",
      "[656]\tvalid_0's rmse: 4.78957\n",
      "[657]\tvalid_0's rmse: 4.78901\n",
      "[658]\tvalid_0's rmse: 4.78824\n",
      "[659]\tvalid_0's rmse: 4.78744\n",
      "[660]\tvalid_0's rmse: 4.78693\n",
      "[661]\tvalid_0's rmse: 4.78573\n",
      "[662]\tvalid_0's rmse: 4.78322\n",
      "[663]\tvalid_0's rmse: 4.78257\n",
      "[664]\tvalid_0's rmse: 4.78187\n",
      "[665]\tvalid_0's rmse: 4.77986\n",
      "[666]\tvalid_0's rmse: 4.77932\n",
      "[667]\tvalid_0's rmse: 4.77872\n",
      "[668]\tvalid_0's rmse: 4.77795\n",
      "[669]\tvalid_0's rmse: 4.77729\n",
      "[670]\tvalid_0's rmse: 4.77618\n",
      "[671]\tvalid_0's rmse: 4.77547\n",
      "[672]\tvalid_0's rmse: 4.7748\n",
      "[673]\tvalid_0's rmse: 4.77419\n",
      "[674]\tvalid_0's rmse: 4.77121\n",
      "[675]\tvalid_0's rmse: 4.77039\n",
      "[676]\tvalid_0's rmse: 4.76799\n",
      "[677]\tvalid_0's rmse: 4.76517\n",
      "[678]\tvalid_0's rmse: 4.76324\n",
      "[679]\tvalid_0's rmse: 4.76264\n",
      "[680]\tvalid_0's rmse: 4.76205\n",
      "[681]\tvalid_0's rmse: 4.76152\n",
      "[682]\tvalid_0's rmse: 4.76102\n",
      "[683]\tvalid_0's rmse: 4.76038\n",
      "[684]\tvalid_0's rmse: 4.75977\n",
      "[685]\tvalid_0's rmse: 4.75921\n",
      "[686]\tvalid_0's rmse: 4.75868\n",
      "[687]\tvalid_0's rmse: 4.75814\n",
      "[688]\tvalid_0's rmse: 4.75711\n",
      "[689]\tvalid_0's rmse: 4.7565\n",
      "[690]\tvalid_0's rmse: 4.75462\n",
      "[691]\tvalid_0's rmse: 4.75195\n",
      "[692]\tvalid_0's rmse: 4.75131\n",
      "[693]\tvalid_0's rmse: 4.75093\n",
      "[694]\tvalid_0's rmse: 4.75033\n",
      "[695]\tvalid_0's rmse: 4.74979\n",
      "[696]\tvalid_0's rmse: 4.74933\n",
      "[697]\tvalid_0's rmse: 4.74875\n",
      "[698]\tvalid_0's rmse: 4.74821\n",
      "[699]\tvalid_0's rmse: 4.7476\n",
      "[700]\tvalid_0's rmse: 4.74705\n",
      "[701]\tvalid_0's rmse: 4.74643\n",
      "[702]\tvalid_0's rmse: 4.74381\n",
      "[703]\tvalid_0's rmse: 4.74119\n",
      "[704]\tvalid_0's rmse: 4.74054\n",
      "[705]\tvalid_0's rmse: 4.74013\n",
      "[706]\tvalid_0's rmse: 4.73966\n",
      "[707]\tvalid_0's rmse: 4.73923\n",
      "[708]\tvalid_0's rmse: 4.73661\n",
      "[709]\tvalid_0's rmse: 4.73426\n",
      "[710]\tvalid_0's rmse: 4.73231\n",
      "[711]\tvalid_0's rmse: 4.73185\n",
      "[712]\tvalid_0's rmse: 4.73134\n",
      "[713]\tvalid_0's rmse: 4.7308\n",
      "[714]\tvalid_0's rmse: 4.72864\n",
      "[715]\tvalid_0's rmse: 4.72732\n",
      "[716]\tvalid_0's rmse: 4.72691\n",
      "[717]\tvalid_0's rmse: 4.72627\n",
      "[718]\tvalid_0's rmse: 4.72498\n",
      "[719]\tvalid_0's rmse: 4.7245\n",
      "[720]\tvalid_0's rmse: 4.72409\n",
      "[721]\tvalid_0's rmse: 4.72365\n",
      "[722]\tvalid_0's rmse: 4.72316\n",
      "[723]\tvalid_0's rmse: 4.72273\n",
      "[724]\tvalid_0's rmse: 4.72221\n",
      "[725]\tvalid_0's rmse: 4.72167\n",
      "[726]\tvalid_0's rmse: 4.71971\n",
      "[727]\tvalid_0's rmse: 4.71855\n",
      "[728]\tvalid_0's rmse: 4.71816\n",
      "[729]\tvalid_0's rmse: 4.71689\n",
      "[730]\tvalid_0's rmse: 4.71564\n",
      "[731]\tvalid_0's rmse: 4.71532\n",
      "[732]\tvalid_0's rmse: 4.71427\n",
      "[733]\tvalid_0's rmse: 4.71353\n",
      "[734]\tvalid_0's rmse: 4.71308\n",
      "[735]\tvalid_0's rmse: 4.71259\n",
      "[736]\tvalid_0's rmse: 4.71136\n",
      "[737]\tvalid_0's rmse: 4.71031\n",
      "[738]\tvalid_0's rmse: 4.70972\n",
      "[739]\tvalid_0's rmse: 4.70929\n",
      "[740]\tvalid_0's rmse: 4.70717\n",
      "[741]\tvalid_0's rmse: 4.70566\n",
      "[742]\tvalid_0's rmse: 4.70512\n",
      "[743]\tvalid_0's rmse: 4.70461\n",
      "[744]\tvalid_0's rmse: 4.70395\n",
      "[745]\tvalid_0's rmse: 4.70189\n",
      "[746]\tvalid_0's rmse: 4.7016\n",
      "[747]\tvalid_0's rmse: 4.69974\n",
      "[748]\tvalid_0's rmse: 4.69896\n",
      "[749]\tvalid_0's rmse: 4.69826\n",
      "[750]\tvalid_0's rmse: 4.69776\n",
      "[751]\tvalid_0's rmse: 4.69733\n",
      "[752]\tvalid_0's rmse: 4.69686\n",
      "[753]\tvalid_0's rmse: 4.69659\n",
      "[754]\tvalid_0's rmse: 4.69603\n",
      "[755]\tvalid_0's rmse: 4.69502\n",
      "[756]\tvalid_0's rmse: 4.69472\n",
      "[757]\tvalid_0's rmse: 4.69394\n",
      "[758]\tvalid_0's rmse: 4.69292\n",
      "[759]\tvalid_0's rmse: 4.69155\n",
      "[760]\tvalid_0's rmse: 4.6911\n",
      "[761]\tvalid_0's rmse: 4.68959\n",
      "[762]\tvalid_0's rmse: 4.6884\n",
      "[763]\tvalid_0's rmse: 4.68739\n",
      "[764]\tvalid_0's rmse: 4.68675\n",
      "[765]\tvalid_0's rmse: 4.68521\n",
      "[766]\tvalid_0's rmse: 4.68433\n",
      "[767]\tvalid_0's rmse: 4.68409\n",
      "[768]\tvalid_0's rmse: 4.68243\n",
      "[769]\tvalid_0's rmse: 4.68104\n",
      "[770]\tvalid_0's rmse: 4.68037\n",
      "[771]\tvalid_0's rmse: 4.67909\n",
      "[772]\tvalid_0's rmse: 4.67718\n",
      "[773]\tvalid_0's rmse: 4.67571\n",
      "[774]\tvalid_0's rmse: 4.6743\n",
      "[775]\tvalid_0's rmse: 4.67283\n",
      "[776]\tvalid_0's rmse: 4.67111\n",
      "[777]\tvalid_0's rmse: 4.66909\n",
      "[778]\tvalid_0's rmse: 4.66777\n",
      "[779]\tvalid_0's rmse: 4.66632\n",
      "[780]\tvalid_0's rmse: 4.66496\n",
      "[781]\tvalid_0's rmse: 4.66354\n",
      "[782]\tvalid_0's rmse: 4.66224\n",
      "[783]\tvalid_0's rmse: 4.66021\n",
      "[784]\tvalid_0's rmse: 4.65871\n",
      "[785]\tvalid_0's rmse: 4.6578\n",
      "[786]\tvalid_0's rmse: 4.65644\n",
      "[787]\tvalid_0's rmse: 4.65492\n",
      "[788]\tvalid_0's rmse: 4.65343\n",
      "[789]\tvalid_0's rmse: 4.65208\n",
      "[790]\tvalid_0's rmse: 4.65062\n",
      "[791]\tvalid_0's rmse: 4.64935\n",
      "[792]\tvalid_0's rmse: 4.64833\n",
      "[793]\tvalid_0's rmse: 4.6468\n",
      "[794]\tvalid_0's rmse: 4.64629\n",
      "[795]\tvalid_0's rmse: 4.64499\n",
      "[796]\tvalid_0's rmse: 4.64338\n",
      "[797]\tvalid_0's rmse: 4.64202\n",
      "[798]\tvalid_0's rmse: 4.64075\n",
      "[799]\tvalid_0's rmse: 4.63973\n",
      "[800]\tvalid_0's rmse: 4.63928\n",
      "[801]\tvalid_0's rmse: 4.63802\n",
      "[802]\tvalid_0's rmse: 4.63685\n",
      "[803]\tvalid_0's rmse: 4.63606\n",
      "[804]\tvalid_0's rmse: 4.63522\n",
      "[805]\tvalid_0's rmse: 4.63379\n",
      "[806]\tvalid_0's rmse: 4.63253\n",
      "[807]\tvalid_0's rmse: 4.6321\n",
      "[808]\tvalid_0's rmse: 4.63136\n",
      "[809]\tvalid_0's rmse: 4.63019\n",
      "[810]\tvalid_0's rmse: 4.62881\n",
      "[811]\tvalid_0's rmse: 4.62821\n",
      "[812]\tvalid_0's rmse: 4.62752\n",
      "[813]\tvalid_0's rmse: 4.62575\n",
      "[814]\tvalid_0's rmse: 4.62472\n",
      "[815]\tvalid_0's rmse: 4.6243\n",
      "[816]\tvalid_0's rmse: 4.62319\n",
      "[817]\tvalid_0's rmse: 4.62205\n",
      "[818]\tvalid_0's rmse: 4.62009\n",
      "[819]\tvalid_0's rmse: 4.61889\n",
      "[820]\tvalid_0's rmse: 4.61787\n",
      "[821]\tvalid_0's rmse: 4.61678\n",
      "[822]\tvalid_0's rmse: 4.61567\n",
      "[823]\tvalid_0's rmse: 4.61422\n",
      "[824]\tvalid_0's rmse: 4.61298\n",
      "[825]\tvalid_0's rmse: 4.61221\n",
      "[826]\tvalid_0's rmse: 4.61118\n",
      "[827]\tvalid_0's rmse: 4.61041\n",
      "[828]\tvalid_0's rmse: 4.60995\n",
      "[829]\tvalid_0's rmse: 4.60887\n",
      "[830]\tvalid_0's rmse: 4.60759\n",
      "[831]\tvalid_0's rmse: 4.60685\n",
      "[832]\tvalid_0's rmse: 4.60526\n",
      "[833]\tvalid_0's rmse: 4.60419\n",
      "[834]\tvalid_0's rmse: 4.6034\n",
      "[835]\tvalid_0's rmse: 4.60265\n",
      "[836]\tvalid_0's rmse: 4.60115\n",
      "[837]\tvalid_0's rmse: 4.59983\n",
      "[838]\tvalid_0's rmse: 4.59868\n",
      "[839]\tvalid_0's rmse: 4.59794\n",
      "[840]\tvalid_0's rmse: 4.59744\n",
      "[841]\tvalid_0's rmse: 4.59707\n",
      "[842]\tvalid_0's rmse: 4.59641\n",
      "[843]\tvalid_0's rmse: 4.59538\n",
      "[844]\tvalid_0's rmse: 4.59487\n",
      "[845]\tvalid_0's rmse: 4.59433\n",
      "[846]\tvalid_0's rmse: 4.59332\n",
      "[847]\tvalid_0's rmse: 4.59253\n",
      "[848]\tvalid_0's rmse: 4.59194\n",
      "[849]\tvalid_0's rmse: 4.59118\n",
      "[850]\tvalid_0's rmse: 4.59018\n",
      "[851]\tvalid_0's rmse: 4.58865\n",
      "[852]\tvalid_0's rmse: 4.58762\n",
      "[853]\tvalid_0's rmse: 4.58686\n",
      "[854]\tvalid_0's rmse: 4.58617\n",
      "[855]\tvalid_0's rmse: 4.58462\n",
      "[856]\tvalid_0's rmse: 4.58398\n",
      "[857]\tvalid_0's rmse: 4.58254\n",
      "[858]\tvalid_0's rmse: 4.58128\n",
      "[859]\tvalid_0's rmse: 4.58034\n",
      "[860]\tvalid_0's rmse: 4.57926\n",
      "[861]\tvalid_0's rmse: 4.57856\n",
      "[862]\tvalid_0's rmse: 4.5774\n",
      "[863]\tvalid_0's rmse: 4.57642\n",
      "[864]\tvalid_0's rmse: 4.57557\n",
      "[865]\tvalid_0's rmse: 4.57424\n",
      "[866]\tvalid_0's rmse: 4.57369\n",
      "[867]\tvalid_0's rmse: 4.57247\n",
      "[868]\tvalid_0's rmse: 4.57166\n",
      "[869]\tvalid_0's rmse: 4.57026\n",
      "[870]\tvalid_0's rmse: 4.56905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[871]\tvalid_0's rmse: 4.56784\n",
      "[872]\tvalid_0's rmse: 4.56648\n",
      "[873]\tvalid_0's rmse: 4.56498\n",
      "[874]\tvalid_0's rmse: 4.5643\n",
      "[875]\tvalid_0's rmse: 4.56315\n",
      "[876]\tvalid_0's rmse: 4.56238\n",
      "[877]\tvalid_0's rmse: 4.56124\n",
      "[878]\tvalid_0's rmse: 4.55962\n",
      "[879]\tvalid_0's rmse: 4.5589\n",
      "[880]\tvalid_0's rmse: 4.55776\n",
      "[881]\tvalid_0's rmse: 4.55663\n",
      "[882]\tvalid_0's rmse: 4.55554\n",
      "[883]\tvalid_0's rmse: 4.55438\n",
      "[884]\tvalid_0's rmse: 4.55267\n",
      "[885]\tvalid_0's rmse: 4.55123\n",
      "[886]\tvalid_0's rmse: 4.55063\n",
      "[887]\tvalid_0's rmse: 4.54947\n",
      "[888]\tvalid_0's rmse: 4.54868\n",
      "[889]\tvalid_0's rmse: 4.5478\n",
      "[890]\tvalid_0's rmse: 4.54682\n",
      "[891]\tvalid_0's rmse: 4.54625\n",
      "[892]\tvalid_0's rmse: 4.54533\n",
      "[893]\tvalid_0's rmse: 4.54392\n",
      "[894]\tvalid_0's rmse: 4.54265\n",
      "[895]\tvalid_0's rmse: 4.54183\n",
      "[896]\tvalid_0's rmse: 4.54075\n",
      "[897]\tvalid_0's rmse: 4.54027\n",
      "[898]\tvalid_0's rmse: 4.53888\n",
      "[899]\tvalid_0's rmse: 4.53841\n",
      "[900]\tvalid_0's rmse: 4.53704\n",
      "[901]\tvalid_0's rmse: 4.53669\n",
      "[902]\tvalid_0's rmse: 4.53639\n",
      "[903]\tvalid_0's rmse: 4.53608\n",
      "[904]\tvalid_0's rmse: 4.53471\n",
      "[905]\tvalid_0's rmse: 4.53438\n",
      "[906]\tvalid_0's rmse: 4.53408\n",
      "[907]\tvalid_0's rmse: 4.53378\n",
      "[908]\tvalid_0's rmse: 4.53342\n",
      "[909]\tvalid_0's rmse: 4.53257\n",
      "[910]\tvalid_0's rmse: 4.53227\n",
      "[911]\tvalid_0's rmse: 4.53201\n",
      "[912]\tvalid_0's rmse: 4.53168\n",
      "[913]\tvalid_0's rmse: 4.53123\n",
      "[914]\tvalid_0's rmse: 4.53083\n",
      "[915]\tvalid_0's rmse: 4.53054\n",
      "[916]\tvalid_0's rmse: 4.52957\n",
      "[917]\tvalid_0's rmse: 4.52908\n",
      "[918]\tvalid_0's rmse: 4.52872\n",
      "[919]\tvalid_0's rmse: 4.52854\n",
      "[920]\tvalid_0's rmse: 4.52816\n",
      "[921]\tvalid_0's rmse: 4.52775\n",
      "[922]\tvalid_0's rmse: 4.52632\n",
      "[923]\tvalid_0's rmse: 4.52486\n",
      "[924]\tvalid_0's rmse: 4.52446\n",
      "[925]\tvalid_0's rmse: 4.52408\n",
      "[926]\tvalid_0's rmse: 4.52371\n",
      "[927]\tvalid_0's rmse: 4.52353\n",
      "[928]\tvalid_0's rmse: 4.52256\n",
      "[929]\tvalid_0's rmse: 4.52233\n",
      "[930]\tvalid_0's rmse: 4.52189\n",
      "[931]\tvalid_0's rmse: 4.52096\n",
      "[932]\tvalid_0's rmse: 4.52047\n",
      "[933]\tvalid_0's rmse: 4.51951\n",
      "[934]\tvalid_0's rmse: 4.51894\n",
      "[935]\tvalid_0's rmse: 4.51793\n",
      "[936]\tvalid_0's rmse: 4.51778\n",
      "[937]\tvalid_0's rmse: 4.51644\n",
      "[938]\tvalid_0's rmse: 4.51506\n",
      "[939]\tvalid_0's rmse: 4.51448\n",
      "[940]\tvalid_0's rmse: 4.51354\n",
      "[941]\tvalid_0's rmse: 4.51267\n",
      "[942]\tvalid_0's rmse: 4.51218\n",
      "[943]\tvalid_0's rmse: 4.51058\n",
      "[944]\tvalid_0's rmse: 4.51029\n",
      "[945]\tvalid_0's rmse: 4.50939\n",
      "[946]\tvalid_0's rmse: 4.50839\n",
      "[947]\tvalid_0's rmse: 4.50697\n",
      "[948]\tvalid_0's rmse: 4.50685\n",
      "[949]\tvalid_0's rmse: 4.50612\n",
      "[950]\tvalid_0's rmse: 4.50553\n",
      "[951]\tvalid_0's rmse: 4.50476\n",
      "[952]\tvalid_0's rmse: 4.50407\n",
      "[953]\tvalid_0's rmse: 4.50326\n",
      "[954]\tvalid_0's rmse: 4.50262\n",
      "[955]\tvalid_0's rmse: 4.50206\n",
      "[956]\tvalid_0's rmse: 4.50105\n",
      "[957]\tvalid_0's rmse: 4.50042\n",
      "[958]\tvalid_0's rmse: 4.49948\n",
      "[959]\tvalid_0's rmse: 4.49878\n",
      "[960]\tvalid_0's rmse: 4.49787\n",
      "[961]\tvalid_0's rmse: 4.49715\n",
      "[962]\tvalid_0's rmse: 4.4966\n",
      "[963]\tvalid_0's rmse: 4.49582\n",
      "[964]\tvalid_0's rmse: 4.49561\n",
      "[965]\tvalid_0's rmse: 4.49429\n",
      "[966]\tvalid_0's rmse: 4.49363\n",
      "[967]\tvalid_0's rmse: 4.49219\n",
      "[968]\tvalid_0's rmse: 4.49201\n",
      "[969]\tvalid_0's rmse: 4.49096\n",
      "[970]\tvalid_0's rmse: 4.49029\n",
      "[971]\tvalid_0's rmse: 4.48949\n",
      "[972]\tvalid_0's rmse: 4.48828\n",
      "[973]\tvalid_0's rmse: 4.48757\n",
      "[974]\tvalid_0's rmse: 4.48731\n",
      "[975]\tvalid_0's rmse: 4.48662\n",
      "[976]\tvalid_0's rmse: 4.4863\n",
      "[977]\tvalid_0's rmse: 4.48565\n",
      "[978]\tvalid_0's rmse: 4.48536\n",
      "[979]\tvalid_0's rmse: 4.48434\n",
      "[980]\tvalid_0's rmse: 4.48373\n",
      "[981]\tvalid_0's rmse: 4.48278\n",
      "[982]\tvalid_0's rmse: 4.48215\n",
      "[983]\tvalid_0's rmse: 4.4815\n",
      "[984]\tvalid_0's rmse: 4.48106\n",
      "[985]\tvalid_0's rmse: 4.48003\n",
      "[986]\tvalid_0's rmse: 4.4795\n",
      "[987]\tvalid_0's rmse: 4.47903\n",
      "[988]\tvalid_0's rmse: 4.47819\n",
      "[989]\tvalid_0's rmse: 4.47747\n",
      "[990]\tvalid_0's rmse: 4.47607\n",
      "[991]\tvalid_0's rmse: 4.47532\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[991]\tvalid_0's rmse: 4.47532\n"
     ]
    }
   ],
   "source": [
    "hyper_params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.005,\n",
    "    'feature_fraction': 0.78,\n",
    "    'bagging_fraction': 1.0,\n",
    "    'bagging_freq': 5,\n",
    "    \"max_depth\": 17,\n",
    "    \"num_leaves\": 50,  \n",
    "    \"n_estimators\": 991\n",
    "}\n",
    "\n",
    "gbm = lgb.LGBMRegressor(**hyper_params)\n",
    "gbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], eval_metric='neg_mean_squared_error', early_stopping_rounds=1000)\n",
    "y_pred = gbm.predict(test_workday_temp, num_iteration=gbm.best_iteration_)\n",
    "\n",
    "\n",
    "test_workday_temp['speed'] = y_pred\n",
    "day_pred = test_workday_temp['speed']\n",
    "df_test = ts_temp.join(day_pred)\n",
    "df_test = df_test.set_index('id')\n",
    "\n",
    "df_test.to_csv(\"test.csv\", columns=[\"speed\"], index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
